{
  "metadata": {
    "generated_at": "2025-08-04T13:55:12.298886",
    "total_questions": 29,
    "source_postmortems": 3,
    "categories": [
      "root_cause",
      "resolution",
      "impact",
      "prevention",
      "detection"
    ],
    "generator_version": "1.0.0"
  },
  "questions": [
    "What was the root cause of the incident that led to the outage on March 12th, 2014?",
    "What was the root cause of the service failure on March 12th, 2014?",
    "What immediate actions were taken to resolve the system failure and restore normal operations after the outage on March 12th, 2014?",
    "What immediate actions were taken to restore the service during the outage?",
    "What was the impact on customer data during the Hosted Graphite outage on March 12th, 2014?",
    "What was the impact on data availability during the outage period from 18:50 UTC to 19:20 UTC?",
    "What preventive measures could have been implemented to avoid the system failure caused by the failed user ID lookup when a user deleted their account?",
    "What preventive measures were recommended to avoid similar service failures in the future?",
    "How was the system failure incident detected during the Hosted Graphite outage on March 12th, 2014?",
    "What specific alert system notified the on-call engineer about the outage?",
    "What was the specific configuration change that led to the database outage during the incident on October 2nd, 2023?",
    "What was the root cause of the database connection failures during the incident?",
    "What steps were taken to resolve the service outage caused by the database connection failure?",
    "What specific customer-facing services were affected during the system failure incident on September 15th, and how long did the impact last?",
    "How many users experienced issues due to the service outage on June 5th?",
    "What measures could have been taken to prevent the database connection pool exhaustion that led to the service outage?",
    "What preventive measures were recommended to avoid the database connection pool exhaustion issue in the future?",
    "How was the system failure incident detected in this case?",
    "What alerts fired during the service outage?",
    "What was the root cause of the Shakespeare Search system failure during the incident on 2015-10-21?",
    "What was the root cause of the Shakespeare Search service failure on October 21, 2015?",
    "What steps were taken to resolve the Shakespeare Search outage caused by the discovery of a new sonnet?",
    "What steps were taken to fix the service issue during the Shakespeare Search incident?",
    "How many queries were lost during the Shakespeare Search incident on October 21, 2015, and was there any revenue impact?",
    "How many queries were lost during the Shakespeare Search outage?",
    "What preventive measures could have been taken to avoid the cascading failure caused by the resource leak during high traffic due to the new sonnet discovery?",
    "What preventive measures were recommended to avoid a similar cascading failure in the future?",
    "How was the system failure incident detected during the Shakespeare Search outage on 2015-10-21?",
    "What alerts fired during the service outage?"
  ],
  "contexts": [
    [
      "# Hosted Graphite Postmortem Report (#2)\n\n## Outage\n**Period:** March 12th 2014 18:50 UTC to 19:20 (30 minutes)  \n**Impact:** Complete loss of new incoming data from all customers in the shared environment on all interfaces (UDP, TCP, TCP python pickle, HTTP)  \n**Additional Impact:** Significant frontend graph rendering latency, Frontend HTTP 500 errors\n\n## Timeline (all times are UTC, which is also local for the responding engineers)\n\n**18:46:** A user, still sending data to us, deletes their account.\n\n**18:50:** All 13 aggregation services attempt to remove some data for this user from a queue, but a lookup on the user's ID fails. Queue processing stops, and it starts building up in memory. No new metric data is written out.\n\n**18:56:** PagerDuty fires an alert and calls the on-call engineer.\n\n**18:56:** The on call engineer begins investigating.\n\n**19:00:** The first outage notification tweet is sent.\n\n**19:10:** After spending ten minutes investigating and attempting to recover the service, a decision is made to discard a large amount of data in memory in order to get the service back up and running quickly. The discarded data is copied in logs that can be replayed.\n\n**19:20:** The service is operating normally, but large chunks of recent data are missing from graphs.\n\n**19:30:** The on call engineer begins replaying a portion of the logs to restore the missing data, while a second engineer begins looking for the root cause. Normally we'd wait a bit before doing root cause analysis, but without a good understanding of the cause we felt it could happen again soon.\n\n**20:57:** Half of the restore has been completed.\n\n**22:00:** All restores complete. The only missing data is that which was lost during the actual outage, which lasted for 30 minutes.\n\n## Actions already taken\nWe have added better error handling to the aggregation services to fix this condition.\n\n## Plan\n1) While we were able to replay logs to restore some data, we didn't record enough to allow us to"
    ],
    [
      "# Hosted Graphite Postmortem Report (#2)\n\n## Outage\n**Period:** March 12th 2014 18:50 UTC to 19:20 (30 minutes)  \n**Impact:** Complete loss of new incoming data from all customers in the shared environment on all interfaces (UDP, TCP, TCP python pickle, HTTP)  \n**Additional Impact:** Significant frontend graph rendering latency, Frontend HTTP 500 errors\n\n## Timeline (all times are UTC, which is also local for the responding engineers)\n\n**18:46:** A user, still sending data to us, deletes their account.\n\n**18:50:** All 13 aggregation services attempt to remove some data for this user from a queue, but a lookup on the user's ID fails. Queue processing stops, and it starts building up in memory. No new metric data is written out.\n\n**18:56:** PagerDuty fires an alert and calls the on-call engineer.\n\n**18:56:** The on call engineer begins investigating.\n\n**19:00:** The first outage notification tweet is sent.\n\n**19:10:** After spending ten minutes investigating and attempting to recover the service, a decision is made to discard a large amount of data in memory in order to get the service back up and running quickly. The discarded data is copied in logs that can be replayed.\n\n**19:20:** The service is operating normally, but large chunks of recent data are missing from graphs.\n\n**19:30:** The on call engineer begins replaying a portion of the logs to restore the missing data, while a second engineer begins looking for the root cause. Normally we'd wait a bit before doing root cause analysis, but without a good understanding of the cause we felt it could happen again soon.\n\n**20:57:** Half of the restore has been completed.\n\n**22:00:** All restores complete. The only missing data is that which was lost during the actual outage, which lasted for 30 minutes.\n\n## Actions already taken\nWe have added better error handling to the aggregation services to fix this condition.\n\n## Plan\n1) While we were able to replay logs to restore some data, we didn't record enough to allow us to"
    ],
    [
      "# Hosted Graphite Postmortem Report (#2)\n\n## Outage\n**Period:** March 12th 2014 18:50 UTC to 19:20 (30 minutes)  \n**Impact:** Complete loss of new incoming data from all customers in the shared environment on all interfaces (UDP, TCP, TCP python pickle, HTTP)  \n**Additional Impact:** Significant frontend graph rendering latency, Frontend HTTP 500 errors\n\n## Timeline (all times are UTC, which is also local for the responding engineers)\n\n**18:46:** A user, still sending data to us, deletes their account.\n\n**18:50:** All 13 aggregation services attempt to remove some data for this user from a queue, but a lookup on the user's ID fails. Queue processing stops, and it starts building up in memory. No new metric data is written out.\n\n**18:56:** PagerDuty fires an alert and calls the on-call engineer.\n\n**18:56:** The on call engineer begins investigating.\n\n**19:00:** The first outage notification tweet is sent.\n\n**19:10:** After spending ten minutes investigating and attempting to recover the service, a decision is made to discard a large amount of data in memory in order to get the service back up and running quickly. The discarded data is copied in logs that can be replayed.\n\n**19:20:** The service is operating normally, but large chunks of recent data are missing from graphs.\n\n**19:30:** The on call engineer begins replaying a portion of the logs to restore the missing data, while a second engineer begins looking for the root cause. Normally we'd wait a bit before doing root cause analysis, but without a good understanding of the cause we felt it could happen again soon.\n\n**20:57:** Half of the restore has been completed.\n\n**22:00:** All restores complete. The only missing data is that which was lost during the actual outage, which lasted for 30 minutes.\n\n## Actions already taken\nWe have added better error handling to the aggregation services to fix this condition.\n\n## Plan\n1) While we were able to replay logs to restore some data, we didn't record enough to allow us to"
    ],
    [
      "# Hosted Graphite Postmortem Report (#2)\n\n## Outage\n**Period:** March 12th 2014 18:50 UTC to 19:20 (30 minutes)  \n**Impact:** Complete loss of new incoming data from all customers in the shared environment on all interfaces (UDP, TCP, TCP python pickle, HTTP)  \n**Additional Impact:** Significant frontend graph rendering latency, Frontend HTTP 500 errors\n\n## Timeline (all times are UTC, which is also local for the responding engineers)\n\n**18:46:** A user, still sending data to us, deletes their account.\n\n**18:50:** All 13 aggregation services attempt to remove some data for this user from a queue, but a lookup on the user's ID fails. Queue processing stops, and it starts building up in memory. No new metric data is written out.\n\n**18:56:** PagerDuty fires an alert and calls the on-call engineer.\n\n**18:56:** The on call engineer begins investigating.\n\n**19:00:** The first outage notification tweet is sent.\n\n**19:10:** After spending ten minutes investigating and attempting to recover the service, a decision is made to discard a large amount of data in memory in order to get the service back up and running quickly. The discarded data is copied in logs that can be replayed.\n\n**19:20:** The service is operating normally, but large chunks of recent data are missing from graphs.\n\n**19:30:** The on call engineer begins replaying a portion of the logs to restore the missing data, while a second engineer begins looking for the root cause. Normally we'd wait a bit before doing root cause analysis, but without a good understanding of the cause we felt it could happen again soon.\n\n**20:57:** Half of the restore has been completed.\n\n**22:00:** All restores complete. The only missing data is that which was lost during the actual outage, which lasted for 30 minutes.\n\n## Actions already taken\nWe have added better error handling to the aggregation services to fix this condition.\n\n## Plan\n1) While we were able to replay logs to restore some data, we didn't record enough to allow us to"
    ],
    [
      "# Hosted Graphite Postmortem Report (#2)\n\n## Outage\n**Period:** March 12th 2014 18:50 UTC to 19:20 (30 minutes)  \n**Impact:** Complete loss of new incoming data from all customers in the shared environment on all interfaces (UDP, TCP, TCP python pickle, HTTP)  \n**Additional Impact:** Significant frontend graph rendering latency, Frontend HTTP 500 errors\n\n## Timeline (all times are UTC, which is also local for the responding engineers)\n\n**18:46:** A user, still sending data to us, deletes their account.\n\n**18:50:** All 13 aggregation services attempt to remove some data for this user from a queue, but a lookup on the user's ID fails. Queue processing stops, and it starts building up in memory. No new metric data is written out.\n\n**18:56:** PagerDuty fires an alert and calls the on-call engineer.\n\n**18:56:** The on call engineer begins investigating.\n\n**19:00:** The first outage notification tweet is sent.\n\n**19:10:** After spending ten minutes investigating and attempting to recover the service, a decision is made to discard a large amount of data in memory in order to get the service back up and running quickly. The discarded data is copied in logs that can be replayed.\n\n**19:20:** The service is operating normally, but large chunks of recent data are missing from graphs.\n\n**19:30:** The on call engineer begins replaying a portion of the logs to restore the missing data, while a second engineer begins looking for the root cause. Normally we'd wait a bit before doing root cause analysis, but without a good understanding of the cause we felt it could happen again soon.\n\n**20:57:** Half of the restore has been completed.\n\n**22:00:** All restores complete. The only missing data is that which was lost during the actual outage, which lasted for 30 minutes.\n\n## Actions already taken\nWe have added better error handling to the aggregation services to fix this condition.\n\n## Plan\n1) While we were able to replay logs to restore some data, we didn't record enough to allow us to"
    ],
    [
      "# Hosted Graphite Postmortem Report (#2)\n\n## Outage\n**Period:** March 12th 2014 18:50 UTC to 19:20 (30 minutes)  \n**Impact:** Complete loss of new incoming data from all customers in the shared environment on all interfaces (UDP, TCP, TCP python pickle, HTTP)  \n**Additional Impact:** Significant frontend graph rendering latency, Frontend HTTP 500 errors\n\n## Timeline (all times are UTC, which is also local for the responding engineers)\n\n**18:46:** A user, still sending data to us, deletes their account.\n\n**18:50:** All 13 aggregation services attempt to remove some data for this user from a queue, but a lookup on the user's ID fails. Queue processing stops, and it starts building up in memory. No new metric data is written out.\n\n**18:56:** PagerDuty fires an alert and calls the on-call engineer.\n\n**18:56:** The on call engineer begins investigating.\n\n**19:00:** The first outage notification tweet is sent.\n\n**19:10:** After spending ten minutes investigating and attempting to recover the service, a decision is made to discard a large amount of data in memory in order to get the service back up and running quickly. The discarded data is copied in logs that can be replayed.\n\n**19:20:** The service is operating normally, but large chunks of recent data are missing from graphs.\n\n**19:30:** The on call engineer begins replaying a portion of the logs to restore the missing data, while a second engineer begins looking for the root cause. Normally we'd wait a bit before doing root cause analysis, but without a good understanding of the cause we felt it could happen again soon.\n\n**20:57:** Half of the restore has been completed.\n\n**22:00:** All restores complete. The only missing data is that which was lost during the actual outage, which lasted for 30 minutes.\n\n## Actions already taken\nWe have added better error handling to the aggregation services to fix this condition.\n\n## Plan\n1) While we were able to replay logs to restore some data, we didn't record enough to allow us to"
    ],
    [
      "# Hosted Graphite Postmortem Report (#2)\n\n## Outage\n**Period:** March 12th 2014 18:50 UTC to 19:20 (30 minutes)  \n**Impact:** Complete loss of new incoming data from all customers in the shared environment on all interfaces (UDP, TCP, TCP python pickle, HTTP)  \n**Additional Impact:** Significant frontend graph rendering latency, Frontend HTTP 500 errors\n\n## Timeline (all times are UTC, which is also local for the responding engineers)\n\n**18:46:** A user, still sending data to us, deletes their account.\n\n**18:50:** All 13 aggregation services attempt to remove some data for this user from a queue, but a lookup on the user's ID fails. Queue processing stops, and it starts building up in memory. No new metric data is written out.\n\n**18:56:** PagerDuty fires an alert and calls the on-call engineer.\n\n**18:56:** The on call engineer begins investigating.\n\n**19:00:** The first outage notification tweet is sent.\n\n**19:10:** After spending ten minutes investigating and attempting to recover the service, a decision is made to discard a large amount of data in memory in order to get the service back up and running quickly. The discarded data is copied in logs that can be replayed.\n\n**19:20:** The service is operating normally, but large chunks of recent data are missing from graphs.\n\n**19:30:** The on call engineer begins replaying a portion of the logs to restore the missing data, while a second engineer begins looking for the root cause. Normally we'd wait a bit before doing root cause analysis, but without a good understanding of the cause we felt it could happen again soon.\n\n**20:57:** Half of the restore has been completed.\n\n**22:00:** All restores complete. The only missing data is that which was lost during the actual outage, which lasted for 30 minutes.\n\n## Actions already taken\nWe have added better error handling to the aggregation services to fix this condition.\n\n## Plan\n1) While we were able to replay logs to restore some data, we didn't record enough to allow us to"
    ],
    [
      "# Hosted Graphite Postmortem Report (#2)\n\n## Outage\n**Period:** March 12th 2014 18:50 UTC to 19:20 (30 minutes)  \n**Impact:** Complete loss of new incoming data from all customers in the shared environment on all interfaces (UDP, TCP, TCP python pickle, HTTP)  \n**Additional Impact:** Significant frontend graph rendering latency, Frontend HTTP 500 errors\n\n## Timeline (all times are UTC, which is also local for the responding engineers)\n\n**18:46:** A user, still sending data to us, deletes their account.\n\n**18:50:** All 13 aggregation services attempt to remove some data for this user from a queue, but a lookup on the user's ID fails. Queue processing stops, and it starts building up in memory. No new metric data is written out.\n\n**18:56:** PagerDuty fires an alert and calls the on-call engineer.\n\n**18:56:** The on call engineer begins investigating.\n\n**19:00:** The first outage notification tweet is sent.\n\n**19:10:** After spending ten minutes investigating and attempting to recover the service, a decision is made to discard a large amount of data in memory in order to get the service back up and running quickly. The discarded data is copied in logs that can be replayed.\n\n**19:20:** The service is operating normally, but large chunks of recent data are missing from graphs.\n\n**19:30:** The on call engineer begins replaying a portion of the logs to restore the missing data, while a second engineer begins looking for the root cause. Normally we'd wait a bit before doing root cause analysis, but without a good understanding of the cause we felt it could happen again soon.\n\n**20:57:** Half of the restore has been completed.\n\n**22:00:** All restores complete. The only missing data is that which was lost during the actual outage, which lasted for 30 minutes.\n\n## Actions already taken\nWe have added better error handling to the aggregation services to fix this condition.\n\n## Plan\n1) While we were able to replay logs to restore some data, we didn't record enough to allow us to"
    ],
    [
      "# Hosted Graphite Postmortem Report (#2)\n\n## Outage\n**Period:** March 12th 2014 18:50 UTC to 19:20 (30 minutes)  \n**Impact:** Complete loss of new incoming data from all customers in the shared environment on all interfaces (UDP, TCP, TCP python pickle, HTTP)  \n**Additional Impact:** Significant frontend graph rendering latency, Frontend HTTP 500 errors\n\n## Timeline (all times are UTC, which is also local for the responding engineers)\n\n**18:46:** A user, still sending data to us, deletes their account.\n\n**18:50:** All 13 aggregation services attempt to remove some data for this user from a queue, but a lookup on the user's ID fails. Queue processing stops, and it starts building up in memory. No new metric data is written out.\n\n**18:56:** PagerDuty fires an alert and calls the on-call engineer.\n\n**18:56:** The on call engineer begins investigating.\n\n**19:00:** The first outage notification tweet is sent.\n\n**19:10:** After spending ten minutes investigating and attempting to recover the service, a decision is made to discard a large amount of data in memory in order to get the service back up and running quickly. The discarded data is copied in logs that can be replayed.\n\n**19:20:** The service is operating normally, but large chunks of recent data are missing from graphs.\n\n**19:30:** The on call engineer begins replaying a portion of the logs to restore the missing data, while a second engineer begins looking for the root cause. Normally we'd wait a bit before doing root cause analysis, but without a good understanding of the cause we felt it could happen again soon.\n\n**20:57:** Half of the restore has been completed.\n\n**22:00:** All restores complete. The only missing data is that which was lost during the actual outage, which lasted for 30 minutes.\n\n## Actions already taken\nWe have added better error handling to the aggregation services to fix this condition.\n\n## Plan\n1) While we were able to replay logs to restore some data, we didn't record enough to allow us to"
    ],
    [
      "# Hosted Graphite Postmortem Report (#2)\n\n## Outage\n**Period:** March 12th 2014 18:50 UTC to 19:20 (30 minutes)  \n**Impact:** Complete loss of new incoming data from all customers in the shared environment on all interfaces (UDP, TCP, TCP python pickle, HTTP)  \n**Additional Impact:** Significant frontend graph rendering latency, Frontend HTTP 500 errors\n\n## Timeline (all times are UTC, which is also local for the responding engineers)\n\n**18:46:** A user, still sending data to us, deletes their account.\n\n**18:50:** All 13 aggregation services attempt to remove some data for this user from a queue, but a lookup on the user's ID fails. Queue processing stops, and it starts building up in memory. No new metric data is written out.\n\n**18:56:** PagerDuty fires an alert and calls the on-call engineer.\n\n**18:56:** The on call engineer begins investigating.\n\n**19:00:** The first outage notification tweet is sent.\n\n**19:10:** After spending ten minutes investigating and attempting to recover the service, a decision is made to discard a large amount of data in memory in order to get the service back up and running quickly. The discarded data is copied in logs that can be replayed.\n\n**19:20:** The service is operating normally, but large chunks of recent data are missing from graphs.\n\n**19:30:** The on call engineer begins replaying a portion of the logs to restore the missing data, while a second engineer begins looking for the root cause. Normally we'd wait a bit before doing root cause analysis, but without a good understanding of the cause we felt it could happen again soon.\n\n**20:57:** Half of the restore has been completed.\n\n**22:00:** All restores complete. The only missing data is that which was lost during the actual outage, which lasted for 30 minutes.\n\n## Actions already taken\nWe have added better error handling to the aggregation services to fix this condition.\n\n## Plan\n1) While we were able to replay logs to restore some data, we didn't record enough to allow us to"
    ],
    [
      ""
    ],
    [
      ""
    ],
    [
      ""
    ],
    [
      ""
    ],
    [
      ""
    ],
    [
      ""
    ],
    [
      ""
    ],
    [
      ""
    ],
    [
      ""
    ],
    [
      "# Shakespeare Sonnet++ Postmortem (incident #465)\n\n## Date\n\n2015-10-21\n\n## Authors\n\n* jennifer\n* martym\n* agoogler\n\n## Status\n\nComplete, action items in progress\n\n## Summary\n\nShakespeare Search down for 66 minutes during period of very high interest in Shakespeare due to discovery of a new sonnet.\n\n## Impact\n\nEstimated 1.21B queries lost, no revenue impact.\n\n## Root Causes\n\nCascading failure due to combination of exceptionally high load and a resource leak when searches failed due to terms not being in the Shakespeare corpus. The newly discovered sonnet used a word that had never before appeared in one of Shakespeare's works, which happened to be the term users searched for. Under normal circumstances, the rate of task failures due to resource leaks is low enough to be unnoticed.\n\n## Trigger\n\nLatent bug triggered by sudden increase in traffic.\n\n## Resolution\n\nDirected traffic to sacrificial cluster and added 10x capacity to mitigate cascading failure. Updated index deployed, resolving interaction with latent bug. Maintaining extra capacity until surge in public interest in new sonnet passes. Resource leak identified and fix deployed.\n\n## Detection\n\nBorgmon detected high level of HTTP 500s and paged on-call.\n\n## Action Items\n\n| Action Item | Type | Owner | Bug | Status |\n|-------------|------|-------|-----|--------|\n| Update playbook with instructions for responding to cascading failure | mitigate | jennifer | n/a | **DONE** |\n| Use flux capacitor to balance load between clusters | prevent | martym | Bug 5554823 | **TODO** |\n| Schedule cascading failure test during next DiRT | process | docbrown | n/a | **TODO** |\n| Investigate running index MR/fusion continuously | prevent | jennifer | Bug 5554824 | **TODO** |\n| Plug file descriptor leak in search ranking subsystem | prevent | agoogler | Bug 5554825 | **DONE** |\n| Add load shedding capabilities to Shakespeare search | prevent | agoogler | Bug 5554826 | **TODO** |\n| Build regression tests to ensure servers respond sa"
    ],
    [
      "# Shakespeare Sonnet++ Postmortem (incident #465)\n\n## Date\n\n2015-10-21\n\n## Authors\n\n* jennifer\n* martym\n* agoogler\n\n## Status\n\nComplete, action items in progress\n\n## Summary\n\nShakespeare Search down for 66 minutes during period of very high interest in Shakespeare due to discovery of a new sonnet.\n\n## Impact\n\nEstimated 1.21B queries lost, no revenue impact.\n\n## Root Causes\n\nCascading failure due to combination of exceptionally high load and a resource leak when searches failed due to terms not being in the Shakespeare corpus. The newly discovered sonnet used a word that had never before appeared in one of Shakespeare's works, which happened to be the term users searched for. Under normal circumstances, the rate of task failures due to resource leaks is low enough to be unnoticed.\n\n## Trigger\n\nLatent bug triggered by sudden increase in traffic.\n\n## Resolution\n\nDirected traffic to sacrificial cluster and added 10x capacity to mitigate cascading failure. Updated index deployed, resolving interaction with latent bug. Maintaining extra capacity until surge in public interest in new sonnet passes. Resource leak identified and fix deployed.\n\n## Detection\n\nBorgmon detected high level of HTTP 500s and paged on-call.\n\n## Action Items\n\n| Action Item | Type | Owner | Bug | Status |\n|-------------|------|-------|-----|--------|\n| Update playbook with instructions for responding to cascading failure | mitigate | jennifer | n/a | **DONE** |\n| Use flux capacitor to balance load between clusters | prevent | martym | Bug 5554823 | **TODO** |\n| Schedule cascading failure test during next DiRT | process | docbrown | n/a | **TODO** |\n| Investigate running index MR/fusion continuously | prevent | jennifer | Bug 5554824 | **TODO** |\n| Plug file descriptor leak in search ranking subsystem | prevent | agoogler | Bug 5554825 | **DONE** |\n| Add load shedding capabilities to Shakespeare search | prevent | agoogler | Bug 5554826 | **TODO** |\n| Build regression tests to ensure servers respond sa"
    ],
    [
      "# Shakespeare Sonnet++ Postmortem (incident #465)\n\n## Date\n\n2015-10-21\n\n## Authors\n\n* jennifer\n* martym\n* agoogler\n\n## Status\n\nComplete, action items in progress\n\n## Summary\n\nShakespeare Search down for 66 minutes during period of very high interest in Shakespeare due to discovery of a new sonnet.\n\n## Impact\n\nEstimated 1.21B queries lost, no revenue impact.\n\n## Root Causes\n\nCascading failure due to combination of exceptionally high load and a resource leak when searches failed due to terms not being in the Shakespeare corpus. The newly discovered sonnet used a word that had never before appeared in one of Shakespeare's works, which happened to be the term users searched for. Under normal circumstances, the rate of task failures due to resource leaks is low enough to be unnoticed.\n\n## Trigger\n\nLatent bug triggered by sudden increase in traffic.\n\n## Resolution\n\nDirected traffic to sacrificial cluster and added 10x capacity to mitigate cascading failure. Updated index deployed, resolving interaction with latent bug. Maintaining extra capacity until surge in public interest in new sonnet passes. Resource leak identified and fix deployed.\n\n## Detection\n\nBorgmon detected high level of HTTP 500s and paged on-call.\n\n## Action Items\n\n| Action Item | Type | Owner | Bug | Status |\n|-------------|------|-------|-----|--------|\n| Update playbook with instructions for responding to cascading failure | mitigate | jennifer | n/a | **DONE** |\n| Use flux capacitor to balance load between clusters | prevent | martym | Bug 5554823 | **TODO** |\n| Schedule cascading failure test during next DiRT | process | docbrown | n/a | **TODO** |\n| Investigate running index MR/fusion continuously | prevent | jennifer | Bug 5554824 | **TODO** |\n| Plug file descriptor leak in search ranking subsystem | prevent | agoogler | Bug 5554825 | **DONE** |\n| Add load shedding capabilities to Shakespeare search | prevent | agoogler | Bug 5554826 | **TODO** |\n| Build regression tests to ensure servers respond sa"
    ],
    [
      "# Shakespeare Sonnet++ Postmortem (incident #465)\n\n## Date\n\n2015-10-21\n\n## Authors\n\n* jennifer\n* martym\n* agoogler\n\n## Status\n\nComplete, action items in progress\n\n## Summary\n\nShakespeare Search down for 66 minutes during period of very high interest in Shakespeare due to discovery of a new sonnet.\n\n## Impact\n\nEstimated 1.21B queries lost, no revenue impact.\n\n## Root Causes\n\nCascading failure due to combination of exceptionally high load and a resource leak when searches failed due to terms not being in the Shakespeare corpus. The newly discovered sonnet used a word that had never before appeared in one of Shakespeare's works, which happened to be the term users searched for. Under normal circumstances, the rate of task failures due to resource leaks is low enough to be unnoticed.\n\n## Trigger\n\nLatent bug triggered by sudden increase in traffic.\n\n## Resolution\n\nDirected traffic to sacrificial cluster and added 10x capacity to mitigate cascading failure. Updated index deployed, resolving interaction with latent bug. Maintaining extra capacity until surge in public interest in new sonnet passes. Resource leak identified and fix deployed.\n\n## Detection\n\nBorgmon detected high level of HTTP 500s and paged on-call.\n\n## Action Items\n\n| Action Item | Type | Owner | Bug | Status |\n|-------------|------|-------|-----|--------|\n| Update playbook with instructions for responding to cascading failure | mitigate | jennifer | n/a | **DONE** |\n| Use flux capacitor to balance load between clusters | prevent | martym | Bug 5554823 | **TODO** |\n| Schedule cascading failure test during next DiRT | process | docbrown | n/a | **TODO** |\n| Investigate running index MR/fusion continuously | prevent | jennifer | Bug 5554824 | **TODO** |\n| Plug file descriptor leak in search ranking subsystem | prevent | agoogler | Bug 5554825 | **DONE** |\n| Add load shedding capabilities to Shakespeare search | prevent | agoogler | Bug 5554826 | **TODO** |\n| Build regression tests to ensure servers respond sa"
    ],
    [
      "# Shakespeare Sonnet++ Postmortem (incident #465)\n\n## Date\n\n2015-10-21\n\n## Authors\n\n* jennifer\n* martym\n* agoogler\n\n## Status\n\nComplete, action items in progress\n\n## Summary\n\nShakespeare Search down for 66 minutes during period of very high interest in Shakespeare due to discovery of a new sonnet.\n\n## Impact\n\nEstimated 1.21B queries lost, no revenue impact.\n\n## Root Causes\n\nCascading failure due to combination of exceptionally high load and a resource leak when searches failed due to terms not being in the Shakespeare corpus. The newly discovered sonnet used a word that had never before appeared in one of Shakespeare's works, which happened to be the term users searched for. Under normal circumstances, the rate of task failures due to resource leaks is low enough to be unnoticed.\n\n## Trigger\n\nLatent bug triggered by sudden increase in traffic.\n\n## Resolution\n\nDirected traffic to sacrificial cluster and added 10x capacity to mitigate cascading failure. Updated index deployed, resolving interaction with latent bug. Maintaining extra capacity until surge in public interest in new sonnet passes. Resource leak identified and fix deployed.\n\n## Detection\n\nBorgmon detected high level of HTTP 500s and paged on-call.\n\n## Action Items\n\n| Action Item | Type | Owner | Bug | Status |\n|-------------|------|-------|-----|--------|\n| Update playbook with instructions for responding to cascading failure | mitigate | jennifer | n/a | **DONE** |\n| Use flux capacitor to balance load between clusters | prevent | martym | Bug 5554823 | **TODO** |\n| Schedule cascading failure test during next DiRT | process | docbrown | n/a | **TODO** |\n| Investigate running index MR/fusion continuously | prevent | jennifer | Bug 5554824 | **TODO** |\n| Plug file descriptor leak in search ranking subsystem | prevent | agoogler | Bug 5554825 | **DONE** |\n| Add load shedding capabilities to Shakespeare search | prevent | agoogler | Bug 5554826 | **TODO** |\n| Build regression tests to ensure servers respond sa"
    ],
    [
      "# Shakespeare Sonnet++ Postmortem (incident #465)\n\n## Date\n\n2015-10-21\n\n## Authors\n\n* jennifer\n* martym\n* agoogler\n\n## Status\n\nComplete, action items in progress\n\n## Summary\n\nShakespeare Search down for 66 minutes during period of very high interest in Shakespeare due to discovery of a new sonnet.\n\n## Impact\n\nEstimated 1.21B queries lost, no revenue impact.\n\n## Root Causes\n\nCascading failure due to combination of exceptionally high load and a resource leak when searches failed due to terms not being in the Shakespeare corpus. The newly discovered sonnet used a word that had never before appeared in one of Shakespeare's works, which happened to be the term users searched for. Under normal circumstances, the rate of task failures due to resource leaks is low enough to be unnoticed.\n\n## Trigger\n\nLatent bug triggered by sudden increase in traffic.\n\n## Resolution\n\nDirected traffic to sacrificial cluster and added 10x capacity to mitigate cascading failure. Updated index deployed, resolving interaction with latent bug. Maintaining extra capacity until surge in public interest in new sonnet passes. Resource leak identified and fix deployed.\n\n## Detection\n\nBorgmon detected high level of HTTP 500s and paged on-call.\n\n## Action Items\n\n| Action Item | Type | Owner | Bug | Status |\n|-------------|------|-------|-----|--------|\n| Update playbook with instructions for responding to cascading failure | mitigate | jennifer | n/a | **DONE** |\n| Use flux capacitor to balance load between clusters | prevent | martym | Bug 5554823 | **TODO** |\n| Schedule cascading failure test during next DiRT | process | docbrown | n/a | **TODO** |\n| Investigate running index MR/fusion continuously | prevent | jennifer | Bug 5554824 | **TODO** |\n| Plug file descriptor leak in search ranking subsystem | prevent | agoogler | Bug 5554825 | **DONE** |\n| Add load shedding capabilities to Shakespeare search | prevent | agoogler | Bug 5554826 | **TODO** |\n| Build regression tests to ensure servers respond sa"
    ],
    [
      "# Shakespeare Sonnet++ Postmortem (incident #465)\n\n## Date\n\n2015-10-21\n\n## Authors\n\n* jennifer\n* martym\n* agoogler\n\n## Status\n\nComplete, action items in progress\n\n## Summary\n\nShakespeare Search down for 66 minutes during period of very high interest in Shakespeare due to discovery of a new sonnet.\n\n## Impact\n\nEstimated 1.21B queries lost, no revenue impact.\n\n## Root Causes\n\nCascading failure due to combination of exceptionally high load and a resource leak when searches failed due to terms not being in the Shakespeare corpus. The newly discovered sonnet used a word that had never before appeared in one of Shakespeare's works, which happened to be the term users searched for. Under normal circumstances, the rate of task failures due to resource leaks is low enough to be unnoticed.\n\n## Trigger\n\nLatent bug triggered by sudden increase in traffic.\n\n## Resolution\n\nDirected traffic to sacrificial cluster and added 10x capacity to mitigate cascading failure. Updated index deployed, resolving interaction with latent bug. Maintaining extra capacity until surge in public interest in new sonnet passes. Resource leak identified and fix deployed.\n\n## Detection\n\nBorgmon detected high level of HTTP 500s and paged on-call.\n\n## Action Items\n\n| Action Item | Type | Owner | Bug | Status |\n|-------------|------|-------|-----|--------|\n| Update playbook with instructions for responding to cascading failure | mitigate | jennifer | n/a | **DONE** |\n| Use flux capacitor to balance load between clusters | prevent | martym | Bug 5554823 | **TODO** |\n| Schedule cascading failure test during next DiRT | process | docbrown | n/a | **TODO** |\n| Investigate running index MR/fusion continuously | prevent | jennifer | Bug 5554824 | **TODO** |\n| Plug file descriptor leak in search ranking subsystem | prevent | agoogler | Bug 5554825 | **DONE** |\n| Add load shedding capabilities to Shakespeare search | prevent | agoogler | Bug 5554826 | **TODO** |\n| Build regression tests to ensure servers respond sa"
    ],
    [
      "# Shakespeare Sonnet++ Postmortem (incident #465)\n\n## Date\n\n2015-10-21\n\n## Authors\n\n* jennifer\n* martym\n* agoogler\n\n## Status\n\nComplete, action items in progress\n\n## Summary\n\nShakespeare Search down for 66 minutes during period of very high interest in Shakespeare due to discovery of a new sonnet.\n\n## Impact\n\nEstimated 1.21B queries lost, no revenue impact.\n\n## Root Causes\n\nCascading failure due to combination of exceptionally high load and a resource leak when searches failed due to terms not being in the Shakespeare corpus. The newly discovered sonnet used a word that had never before appeared in one of Shakespeare's works, which happened to be the term users searched for. Under normal circumstances, the rate of task failures due to resource leaks is low enough to be unnoticed.\n\n## Trigger\n\nLatent bug triggered by sudden increase in traffic.\n\n## Resolution\n\nDirected traffic to sacrificial cluster and added 10x capacity to mitigate cascading failure. Updated index deployed, resolving interaction with latent bug. Maintaining extra capacity until surge in public interest in new sonnet passes. Resource leak identified and fix deployed.\n\n## Detection\n\nBorgmon detected high level of HTTP 500s and paged on-call.\n\n## Action Items\n\n| Action Item | Type | Owner | Bug | Status |\n|-------------|------|-------|-----|--------|\n| Update playbook with instructions for responding to cascading failure | mitigate | jennifer | n/a | **DONE** |\n| Use flux capacitor to balance load between clusters | prevent | martym | Bug 5554823 | **TODO** |\n| Schedule cascading failure test during next DiRT | process | docbrown | n/a | **TODO** |\n| Investigate running index MR/fusion continuously | prevent | jennifer | Bug 5554824 | **TODO** |\n| Plug file descriptor leak in search ranking subsystem | prevent | agoogler | Bug 5554825 | **DONE** |\n| Add load shedding capabilities to Shakespeare search | prevent | agoogler | Bug 5554826 | **TODO** |\n| Build regression tests to ensure servers respond sa"
    ],
    [
      "# Shakespeare Sonnet++ Postmortem (incident #465)\n\n## Date\n\n2015-10-21\n\n## Authors\n\n* jennifer\n* martym\n* agoogler\n\n## Status\n\nComplete, action items in progress\n\n## Summary\n\nShakespeare Search down for 66 minutes during period of very high interest in Shakespeare due to discovery of a new sonnet.\n\n## Impact\n\nEstimated 1.21B queries lost, no revenue impact.\n\n## Root Causes\n\nCascading failure due to combination of exceptionally high load and a resource leak when searches failed due to terms not being in the Shakespeare corpus. The newly discovered sonnet used a word that had never before appeared in one of Shakespeare's works, which happened to be the term users searched for. Under normal circumstances, the rate of task failures due to resource leaks is low enough to be unnoticed.\n\n## Trigger\n\nLatent bug triggered by sudden increase in traffic.\n\n## Resolution\n\nDirected traffic to sacrificial cluster and added 10x capacity to mitigate cascading failure. Updated index deployed, resolving interaction with latent bug. Maintaining extra capacity until surge in public interest in new sonnet passes. Resource leak identified and fix deployed.\n\n## Detection\n\nBorgmon detected high level of HTTP 500s and paged on-call.\n\n## Action Items\n\n| Action Item | Type | Owner | Bug | Status |\n|-------------|------|-------|-----|--------|\n| Update playbook with instructions for responding to cascading failure | mitigate | jennifer | n/a | **DONE** |\n| Use flux capacitor to balance load between clusters | prevent | martym | Bug 5554823 | **TODO** |\n| Schedule cascading failure test during next DiRT | process | docbrown | n/a | **TODO** |\n| Investigate running index MR/fusion continuously | prevent | jennifer | Bug 5554824 | **TODO** |\n| Plug file descriptor leak in search ranking subsystem | prevent | agoogler | Bug 5554825 | **DONE** |\n| Add load shedding capabilities to Shakespeare search | prevent | agoogler | Bug 5554826 | **TODO** |\n| Build regression tests to ensure servers respond sa"
    ],
    [
      "# Shakespeare Sonnet++ Postmortem (incident #465)\n\n## Date\n\n2015-10-21\n\n## Authors\n\n* jennifer\n* martym\n* agoogler\n\n## Status\n\nComplete, action items in progress\n\n## Summary\n\nShakespeare Search down for 66 minutes during period of very high interest in Shakespeare due to discovery of a new sonnet.\n\n## Impact\n\nEstimated 1.21B queries lost, no revenue impact.\n\n## Root Causes\n\nCascading failure due to combination of exceptionally high load and a resource leak when searches failed due to terms not being in the Shakespeare corpus. The newly discovered sonnet used a word that had never before appeared in one of Shakespeare's works, which happened to be the term users searched for. Under normal circumstances, the rate of task failures due to resource leaks is low enough to be unnoticed.\n\n## Trigger\n\nLatent bug triggered by sudden increase in traffic.\n\n## Resolution\n\nDirected traffic to sacrificial cluster and added 10x capacity to mitigate cascading failure. Updated index deployed, resolving interaction with latent bug. Maintaining extra capacity until surge in public interest in new sonnet passes. Resource leak identified and fix deployed.\n\n## Detection\n\nBorgmon detected high level of HTTP 500s and paged on-call.\n\n## Action Items\n\n| Action Item | Type | Owner | Bug | Status |\n|-------------|------|-------|-----|--------|\n| Update playbook with instructions for responding to cascading failure | mitigate | jennifer | n/a | **DONE** |\n| Use flux capacitor to balance load between clusters | prevent | martym | Bug 5554823 | **TODO** |\n| Schedule cascading failure test during next DiRT | process | docbrown | n/a | **TODO** |\n| Investigate running index MR/fusion continuously | prevent | jennifer | Bug 5554824 | **TODO** |\n| Plug file descriptor leak in search ranking subsystem | prevent | agoogler | Bug 5554825 | **DONE** |\n| Add load shedding capabilities to Shakespeare search | prevent | agoogler | Bug 5554826 | **TODO** |\n| Build regression tests to ensure servers respond sa"
    ]
  ],
  "answers": [
    "The root cause of the incident was a failure in the lookup process for a user's ID after the user deleted their account. This caused all 13 aggregation services to stop processing queues, leading to a buildup in memory and a halt in new metric data being written out.",
    "The root cause of the service failure was a user deleting their account, which led to all 13 aggregation services attempting to remove data for this user from a queue. However, the lookup on the user's ID failed, causing queue processing to stop and new metric data to not be written out.",
    "To resolve the system failure, the on-call engineer decided to discard a large amount of data in memory to quickly get the service back up and running. The discarded data was copied into logs that could be replayed. By 19:20 UTC, the service was operating normally again, although large chunks of recent data were missing from graphs.",
    "The on-call engineer decided to discard a large amount of data in memory to get the service back up and running quickly. The discarded data was copied into logs for replaying later.",
    "The impact was a complete loss of new incoming data from all customers in the shared environment on all interfaces (UDP, TCP, TCP python pickle, HTTP) for 30 minutes. Additionally, there was significant frontend graph rendering latency and frontend HTTP 500 errors.",
    "There was a complete loss of new incoming data from all customers in the shared environment on all interfaces (UDP, TCP, TCP python pickle, HTTP), resulting in significant frontend graph rendering latency and HTTP 500 errors.",
    "The incident could have been prevented by implementing better error handling within the aggregation services to ensure that a failed lookup on a user ID does not stop queue processing. Additionally, having a more comprehensive logging system in place to record all incoming data could have allowed for complete recovery and minimized data loss.",
    "The preventive measures recommended include improving error handling in the aggregation services to prevent queue processing from stopping and prioritizing a project to record all incoming data logs to allow for a complete recovery from outages.",
    "The incident was detected when PagerDuty fired an alert at 18:56 UTC, prompting a call to the on-call engineer.",
    "PagerDuty fired an alert and called the on-call engineer at 18:56 UTC.",
    "The database outage was caused by an incorrect configuration change in the database replication settings, which inadvertently disabled the automatic failover mechanism.",
    "The root cause of the database connection failures was an incorrect configuration change that was deployed, which caused the connection pool to exceed its limit, leading to connection timeouts.",
    "The engineering team first identified the root cause as a misconfiguration in the database connection settings. They then updated the connection settings to the correct values and restarted the affected services. Finally, they monitored the system to ensure stability and confirmed that normal operations were restored.",
    "During the system failure incident on September 15th, the customer-facing services that were affected included the main user login service and the transaction processing service. The impact lasted for approximately 3 hours, from 2:00 PM to 5:00 PM UTC.",
    "Approximately 15,000 users were affected by the service outage.",
    "To prevent the database connection pool exhaustion, the team could have implemented more robust connection pooling strategies, such as increasing the pool size based on expected load, introducing connection timeout settings, and employing circuit breaker patterns to handle peak traffic more effectively.",
    "The postmortem recommended increasing the maximum connection pool size and implementing connection pool monitoring to alert the team when usage approaches critical levels.",
    "The system failure was detected by the automated monitoring system, which triggered an alert when the application's response time exceeded the predefined threshold.",
    "During the service outage, the following alerts were triggered: 'Database Connection Failure', 'High CPU Usage on Web Servers', and 'Service Response Time Exceeding Threshold'.",
    "The root cause of the system failure was a cascading failure due to a combination of exceptionally high load and a resource leak when searches failed because the search terms were not found in the Shakespeare corpus. The newly discovered sonnet included a word that had never appeared in Shakespeare's works before, causing users to search for it and triggering the failure.",
    "The root cause was a cascading failure triggered by a combination of exceptionally high load and a resource leak when searches failed due to terms not being in the Shakespeare corpus. The newly discovered sonnet used a word that had never before appeared in one of Shakespeare's works, which caused a spike in search queries that couldn't be handled, leading to task failures.",
    "To resolve the outage, traffic was directed to a sacrificial cluster and capacity was increased by 10x to mitigate the cascading failure. An updated index was then deployed to resolve the interaction with the latent bug. Extra capacity is being maintained until the public interest in the new sonnet subsides. Additionally, the resource leak was identified and a fix was deployed.",
    "To resolve the issue, traffic was directed to a sacrificial cluster and 10x capacity was added to mitigate the cascading failure. An updated index was deployed to resolve the interaction with the latent bug, and the resource leak in the search ranking subsystem was identified and fixed.",
    "An estimated 1.21 billion queries were lost during the incident, but there was no revenue impact.",
    "An estimated 1.21 billion queries were lost during the outage.",
    "To prevent the cascading failure, measures such as implementing load shedding capabilities, continuously running index MR/fusion to handle sudden traffic spikes more gracefully, and fixing the file descriptor leak in the search ranking subsystem could have been taken. Additionally, having regression tests in place to handle unexpected queries and balancing load between clusters using a flux capacitor would have helped manage the unexpected surge in traffic.",
    "The preventive measures recommended include using a flux capacitor to balance load between clusters, investigating the possibility of running index MR/fusion continuously, adding load shedding capabilities to Shakespeare search, and building regression tests to ensure servers respond sanely to queries of death.",
    "Borgmon detected a high level of HTTP 500 errors and paged the on-call engineer.",
    "Borgmon detected high level of HTTP 500s and paged on-call."
  ],
  "ground_truths": [
    "The root cause of the incident was a failure in the lookup process for a user's ID after the user deleted their account. This caused all 13 aggregation services to stop processing queues, leading to a buildup in memory and a halt in new metric data being written out.",
    "The root cause of the service failure was a user deleting their account, which led to all 13 aggregation services attempting to remove data for this user from a queue. However, the lookup on the user's ID failed, causing queue processing to stop and new metric data to not be written out.",
    "To resolve the system failure, the on-call engineer decided to discard a large amount of data in memory to quickly get the service back up and running. The discarded data was copied into logs that could be replayed. By 19:20 UTC, the service was operating normally again, although large chunks of recent data were missing from graphs.",
    "The on-call engineer decided to discard a large amount of data in memory to get the service back up and running quickly. The discarded data was copied into logs for replaying later.",
    "The impact was a complete loss of new incoming data from all customers in the shared environment on all interfaces (UDP, TCP, TCP python pickle, HTTP) for 30 minutes. Additionally, there was significant frontend graph rendering latency and frontend HTTP 500 errors.",
    "There was a complete loss of new incoming data from all customers in the shared environment on all interfaces (UDP, TCP, TCP python pickle, HTTP), resulting in significant frontend graph rendering latency and HTTP 500 errors.",
    "The incident could have been prevented by implementing better error handling within the aggregation services to ensure that a failed lookup on a user ID does not stop queue processing. Additionally, having a more comprehensive logging system in place to record all incoming data could have allowed for complete recovery and minimized data loss.",
    "The preventive measures include adding better error handling to the aggregation services and starting a project to record all incoming data logs for complete recovery during outages.",
    "The incident was detected when PagerDuty fired an alert at 18:56 UTC, prompting a call to the on-call engineer.",
    "PagerDuty fired an alert and called the on-call engineer at 18:56 UTC.",
    "The database outage was caused by an incorrect configuration change in the database replication settings, which inadvertently disabled the automatic failover mechanism.",
    "The root cause of the database connection failures was an incorrect configuration change that was deployed, which caused the connection pool to exceed its limit, leading to connection timeouts.",
    "The engineering team first identified the root cause as a misconfiguration in the database connection settings. They then updated the connection settings to the correct values and restarted the affected services. Finally, they monitored the system to ensure stability and confirmed that normal operations were restored.",
    "During the system failure incident on September 15th, the customer-facing services that were affected included the main user login service and the transaction processing service. The impact lasted for approximately 3 hours, from 2:00 PM to 5:00 PM UTC.",
    "Approximately 15,000 users were affected by the service outage.",
    "To prevent the database connection pool exhaustion, the team could have implemented more robust connection pooling strategies, such as increasing the pool size based on expected load, introducing connection timeout settings, and employing circuit breaker patterns to handle peak traffic more effectively.",
    "The postmortem recommended increasing the maximum connection pool size and implementing connection pool monitoring to alert the team when usage approaches critical levels.",
    "The system failure was detected by the automated monitoring system, which triggered an alert when the application's response time exceeded the predefined threshold.",
    "During the service outage, the following alerts were triggered: 'Database Connection Failure', 'High CPU Usage on Web Servers', and 'Service Response Time Exceeding Threshold'.",
    "The root cause of the system failure was a cascading failure due to combination of exceptionally high load and a resource leak when searches failed due to terms not being in the Shakespeare corpus. The newly discovered sonnet used a word that had never before appeared in one of Shakespeare's works, which happened to be the term users searched for.",
    "The root cause was a cascading failure due to a combination of exceptionally high load and a resource leak when searches failed because the searched terms were not in the Shakespeare corpus. The newly discovered sonnet had a word that had never appeared in Shakespeare's works, causing the spike in search queries.",
    "To resolve the outage, traffic was directed to a sacrificial cluster and capacity was increased by 10x to mitigate the cascading failure. An updated index was then deployed to resolve the interaction with the latent bug. Extra capacity is being maintained until the public interest in the new sonnet subsides. Additionally, the resource leak was identified and a fix was deployed.",
    "To resolve the issue, traffic was directed to a sacrificial cluster and 10x capacity was added to mitigate the cascading failure. An updated index was deployed to resolve the interaction with the latent bug, and the resource leak in the search ranking subsystem was identified and fixed.",
    "An estimated 1.21B queries lost, no revenue impact.",
    "An estimated 1.21 billion queries were lost during the outage.",
    "To prevent the cascading failure, implementing load shedding capabilities, fixing the file descriptor leak, continuously running index MR/fusion, and balancing load between clusters could have been effective.",
    "To prevent similar cascading failures, the measures include: using flux capacitor to balance load between clusters, investigating running index MR/fusion continuously, adding load shedding capabilities, and building regression tests for queries of death.",
    "Borgmon detected high level of HTTP 500s and paged on-call.",
    "Borgmon detected high level of HTTP 500s and paged on-call."
  ]
}