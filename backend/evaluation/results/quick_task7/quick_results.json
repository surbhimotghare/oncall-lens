{
  "metadata": {
    "run_name": "quick_task7",
    "timestamp": "2025-08-24T20:55:51.473912",
    "strategies_tested": [
      "naive",
      "hybrid",
      "ensemble"
    ],
    "baseline_comparison": true
  },
  "baseline_scores": {
    "faithfulness": 0.267,
    "answer_relevancy": 0.518,
    "context_precision": 0.75,
    "context_recall": 0.833,
    "semantic_similarity": 0.437,
    "answer_correctness": 0.163
  },
  "strategy_results": {
    "naive": {
      "scores": {
        "faithfulness": 1.0,
        "answer_relevancy": 0.68,
        "context_precision": 1.0,
        "context_recall": 1.0,
        "semantic_similarity": 1.0,
        "answer_correctness": 1.0
      },
      "evaluation_data": {
        "questions": [
          "What was the root cause of the Hosted Graphite outage on March 12th, 2014?",
          "How was the service outage resolved and what steps were taken?"
        ],
        "responses": [
          "The root cause of the Hosted Graphite outage on March 12th, 2014, was a failure in the lookup of a user's ID after the user deleted their account. This caused all 13 aggregation services to stop processing the queue, leading to a buildup in memory and preventing new metric data from being written out.",
          "The service outage was resolved by discarding a large amount of data in memory to quickly get the service back up and running. The discarded data was copied into logs that could be replayed later. The service was operating normally by 19:20 UTC, but large chunks of recent data were missing from graphs. The on-call engineer then began replaying a portion of the logs to restore the missing data, while a second engineer started investigating the root cause. By 22:00 UTC, all restores were complete, with only the data lost during the actual 30-minute outage remaining missing.\n\nSteps taken included:\n1. Discarding data in memory to restart the service quickly.\n2. Copying the discarded data into logs for later replay.\n3. Replaying logs to restore missing data.\n4. Investigating the root cause to prevent future occurrences.\n5. Adding better error handling to the aggregation services.\n6. Planning to record all data to allow complete recovery from future outages.\n7. Planning to publish an offsite status page to keep customers informed during such events."
        ],
        "contexts": [
          [
            "# Hosted Graphite Postmortem Report (#2)\n\n## Outage\n**Period:** March 12th 2014 18:50 UTC to 19:20 (30 minutes)  \n**Impact:** Complete loss of new incoming data from all customers in the shared environment on all interfaces (UDP, TCP, TCP python pickle, HTTP)  \n**Additional Impact:** Significant frontend graph rendering latency, Frontend HTTP 500 errors\n\n## Timeline (all times are UTC, which is also local for the responding engineers)\n\n**18:46:** A user, still sending data to us, deletes their account.\n\n**18:50:** All 13 aggregation services attempt to remove some data for this user from a queue, but a lookup on the user's ID fails. Queue processing stops, and it starts building up in memory. No new metric data is written out.\n\n**18:56:** PagerDuty fires an alert and calls the on-call engineer.\n\n**18:56:** The on call engineer begins investigating.\n\n**19:00:** The first outage notification tweet is sent.\n\n**19:10:** After spending ten minutes investigating and attempting to recover the service, a decision is made to discard a large amount of data in memory in order to get the service back up and running quickly. The discarded data is copied in logs that can be replayed.\n\n**19:20:** The service is operating normally, but large chunks of recent data are missing from graphs.\n\n**19:30:** The on call engineer begins replaying a portion of the logs to restore the missing data, while a second engineer begins looking for the root cause. Normally we'd wait a bit before doing root cause analysis, but without a good understanding of the cause we felt it could happen again soon.\n\n**20:57:** Half of the restore has been completed.\n\n**22:00:** All restores complete. The only missing data is that which was lost during the actual outage, which lasted for 30 minutes.\n\n## Actions already taken\nWe have added better error handling to the aggregation services to fix this condition.\n\n## Plan\n1) While we were able to replay logs to restore some data, we didn't record enough to allow us to replay everything. We've had a project planned for a few months to record everything and hopefully allow complete recovery from outages, and we've changed some project priorities to get that work started next week.\n\n2) We still plan to publish an offsite status page to help keep customers informed during events like this.\n\n## Conclusion\nWe always regret losing data, it sucks and we're sorry we failed this time.\n\nBeing able to replay logs to recover from a failure is a pretty powerful tool. It helped make the decision to discard some data in order to recover the service faster, and we think that with more logging of incoming data that we'll be in a better position the next time we have an outage like this. \n\n> Sample template from [Google Developers Blog: \"Google API infrastructure outage incident report\"](https://developers.googleblog.com/2013/05/google-api-infrastructure-outage_3.html).\n\n# Title\n\n## Issue Summary\n\n## Timeline\n> (all times in Pacific Time)\n\n## Root Cause\n\n## Resolution and recovery\n\n## Corrective and Preventative Measures\n\n\n\n> Template from the [Elastic Cloud](https://www.elastic.co/blog/elastic-cloud-incident-report-feburary-4-2019) incident report.\n\n# Title of the incident: Date\n\nBy: (Names)\n\nBrief description\n\n## Background\n\n## What Happened\n\n### Chain of Events\n\n## Root Cause\n\n## Resolution\n\n## Impact\n\n## Action Items\n\n\n> Template from: Betsy Beyer, Chris Jones, Jennifer Petoff, and Niall Richard Murphy. [\u201cSite Reliability Engineering.\u201d](https://landing.google.com/sre/book/chapters/postmortem.html).\n\n# Title (incident #)\n\n### Date\n\n### Authors\n\n### Status\n\n### Summary\n\n### Impact\n\n### Root Causes\n\n### Trigger\n\n### Resolution\n\n### Detection\n\n## Action Items\n\n## Lessons Learned\n\n### What went well\n\n### What went wrong\n\n### Where we got lucky\n\n## Timeline\n\n## Supporting information\n\n\n> Sample template from: Thomas A. Limoncelli. [\u201cThe Practice of Cloud System Administration: Designing and Operating Large Distributed Systems, Volume 2.\u201d](http://the-cloud-book.com/).\n\n# Title\n\n### Report Status\n\n### Executive Summary\n\n### Outage Description\n\n### Affected users\n\n### Start Date/Time\n\n### End Date/Time\n\n### Duration\n\n## Timeline\n\n## Contributing Conditions Analysis\n\n### What went well?\n\n### What could have gone better?\n\n## Recommendations\n\n### Names of people involved\n\n\n\n# Shakespeare Sonnet++ Postmortem (incident #465)\n\n## Date\n\n2015-10-21\n\n## Authors\n\n* jennifer\n* martym\n* agoogler\n\n## Status\n\nComplete, action items in progress\n\n## Summary\n\nShakespeare Search down for 66 minutes during period of very high interest in Shakespeare due to discovery of a new sonnet.\n\n## Impact\n\nEstimated 1.21B queries lost, no revenue impact.\n\n## Root Causes\n\nCascading failure due to combination of exceptionally high load and a resource leak when searches failed due to terms not being in the Shakespeare corpus. The newly discovered sonnet used a word that had never before appeared in one of Shakespeare's works, which happened to be the term users searched for. Under normal circumstances, the rate of task failures due to resource leaks is low enough to be unnoticed.\n\n## Trigger\n\nLatent bug triggered by sudden increase in traffic.\n\n## Resolution\n\nDirected traffic to sacrificial cluster and added 10x capacity to mitigate cascading failure. Updated index deployed, resolving interaction with latent bug. Maintaining extra capacity until surge in public interest in new sonnet passes. Resource leak identified and fix deployed.\n\n## Detection\n\nBorgmon detected high level of HTTP 500s and paged on-call.\n\n## Action Items\n\n| Action Item | Type | Owner | Bug | Status |\n|-------------|------|-------|-----|--------|\n| Update playbook with instructions for responding to cascading failure | mitigate | jennifer | n/a | **DONE** |\n| Use flux capacitor to balance load between clusters | prevent | martym | Bug 5554823 | **TODO** |\n| Schedule cascading failure test during next DiRT | process | docbrown | n/a | **TODO** |\n| Investigate running index MR/fusion continuously | prevent | jennifer | Bug 5554824 | **TODO** |\n| Plug file descriptor leak in search ranking subsystem | prevent | agoogler | Bug 5554825 | **DONE** |\n| Add load shedding capabilities to Shakespeare search | prevent | agoogler | Bug 5554826 | **TODO** |\n| Build regression tests to ensure servers respond sanely to queries of death | prevent | clarac | Bug 5554827 | **TODO** |\n| Deploy updated search ranking subsystem to prod | prevent | jennifer | n/a | **DONE** |\n| Freeze production until 2015-11-20 due to error budget exhaustion | other | docbrown | n/a | **TODO** |\n\n## Lessons Learned\n\n### What went well\n\n* Monitoring quickly alerted us to high rate (reaching ~100%) of HTTP 500s\n* Rapidly distributed updated Shakespeare corpus to all clusters\n\n### What went wrong\n\n* We're out of practice in responding to cascading failure\n* We exceeded our availability error budget (by several orders of magnitude) due to the exceptional surge of traffic that essentially all resulted in failures\n\n### Where we got lucky\n\n* Mailing list of Shakespeare aficionados had a copy of new sonnet available\n* Server logs had stack traces pointing to file descriptor exhaustion as cause for crash\n* Query-of-death was resolved by pushing new index containing popular search term\n\n## Timeline\n\n2015-10-21 (*all times UTC*)\n\n| Time | Description |\n|------|-------------|\n| 14:51 | News reports that a new Shakespearean sonnet has been discovered in a Delorean's glove compartment |\n| 14:53 | Traffic to Shakespeare search increases by 88x after post to _/r/shakespeare_ points to Shakespeare search engine as place to find new sonnet (except we don't have the sonnet yet) |\n| 14:54 | **OUTAGE BEGINS** -- Search backends start melting down under load |\n| 14:55 | docbrown receives pager storm, ManyHttp500s from all clusters |\n| 14:57 | All traffic to Shakespeare search is failing |\n| 14:58 | docbrown starts investigating, finds backend crash rate very high |\n| 15:01 | **INCIDENT BEGINS** docbrown declares incident #465 due to cascading failure, coordination on #shakespeare, names jennifer incident commander |\n| 15:02 | someone coincidentally sends email to shakespeare-discuss@ re sonnet discovery, which happens to be at top of martym's inbox |\n| 15:03 | jennifer notifies shakespeare-incidents@ list of the incident |\n| 15:04 | martym tracks down text of new sonnet and looks for documentation on corpus update |\n| 15:06 | docbrown finds that crash symptoms identical across all tasks in all clusters, investigating cause based on application logs |\n| 15:07 | martym finds documentation, starts prep work for corpus update |\n| 15:10 | martym adds sonnet to Shakespeare's known works, starts indexing job |\n| 15:12 | docbrown contacts clarac & agoogler (from Shakespeare dev team) to help with examining codebase for possible causes |\n| 15:18 | clarac finds smoking gun in logs pointing to file descriptor exhaustion, confirms against code that leak exists if term not in corpus is searched for |\n| 15:20 | martym's index MapReduce job completes |\n| 15:21 | jennifer and docbrown decide to increase instance count enough to drop load on instances that they're able to do appreciable work before dying and being restarted |\n| 15:23 | docbrown load balances all traffic to USA-2 cluster, permitting instance count increase in other clusters without servers failing immediately |\n| 15:25 | martym starts replicating new index to all clusters |\n| 15:28 | docbrown starts 2x instance count increase |\n| 15:32 | jennifer changes load balancing to increase traffic to nonsacrificial clusters |\n| 15:33 | tasks in nonsacrificial clusters start failing, same symptoms as before |\n| 15:34 | found order-of-magnitude error in whiteboard calculations for instance count increase |\n| 15:36 | jennifer reverts load balancing to resacrifice USA-2 cluster in preparation for additional global 5x instance count increase (to a total of 10x initial capacity) |\n| 15:36 | **OUTAGE MITIGATED**, updated index replicated to all clusters |\n| 15:39 | docbrown starts second wave of instance count increase to 10x initial capacity |\n| 15:41 | jennifer reinstates load balancing across all clusters for 1% of traffic |\n| 15:43 | nonsacrificial clusters' HTTP 500 rates at nominal rates, task failures intermittent at low levels |\n| 15:45 | jennifer balances 10% of traffic across nonsacrificial clusters |\n| 15:47 | nonsacrificial clusters' HTTP 500 rates remain within SLO, no task failures observed |\n| 15:50 | 30% of traffic balanced across nonsacrificial clusters |\n| 15:55 | 50% of traffic balanced across nonsacrificial clusters |\n| 16:00 | **OUTAGE ENDS**, all traffic balanced across all clusters |\n| 16:30 | **INCIDENT ENDS**, reached exit criterion of 30 minutes' nominal performance |\n\n## Supporting Information\n\n* Monitoring dashboard, http://monitor/shakespeare?end%5Ftime=20151021T160000&duration=7200 \n\n> Template from: [Microsoft Azure status history](https://status.azure.com/en-us/status/history/).\n\n# Title (incident)\n\n### Date\n\n### Summary of impact\n\n### Customer impact\n\n### Root cause and mitigation\n\n### Next steps\n\n### Provide feedback (customer survey)\n\n\n> Template from the [Real-world SRE](http://a.co/d/34UsUVU) book.\n\n# Title of the incident\n\nName(s):\n\nDate: \n\nLast modified: \n\n## Summary\n\n## Impact\n\n## Timeline\n\n## Root Cause(s)\n\n## Action Items\n\n## Appendix\n\n\n> Template from: [Michael Kehoe](https://michael-kehoe.io/post/postmortem-template/)\n\n# Postmortem Template\n\n### Summary\n| Incident Summary   |   |                       |   |\n|--------------------|---|-----------------------|---|\n| Incident Number    |   | Incident Severity     |   |\n| Postmortem Date    |   | War-room Required     |   |\n| SRE Lead           |   | Developer Lead        |   |\n| Incident Mgmt Lead |   | Chaos Eng Preventable |   |\n| Postmortem Lead    |   | Recording             |   |\n\n### Postmortem Attendees\n| Name | Role | In attendence |\n|------|------|---------------|\n|      |      |               |\n|      |      |               |\n|      |      |               |\n\n### Incident Timing\n| Start Time      |       | Incident Detected By(User-reported/ Ad-hoc monitoring/ Alerting system) |      |\n|-----------------|-------|-------------------------------------------------------------------------|------|\n| Detection Time  |       | Time to Detect (TTD)                                                    |      |\n| Mitigation Time |       | Time to Mitigate (TTM)                                                  |      |\n| Resolution Time |       | Time to Resolve (TTR)                                                   |      |\n\n### Incident Timeline\n| Date/Time | Who/What | Action/ Impact |\n|-----------|----------|----------------|\n|           |          |                |\n|           |          |                |\n|           |          |                |\n\n### Impact\n\n#### End User Impact\n\n#### Infrastructure Impact\n\n#### Productivity Impact\n\n### What caused the incident?\n\n#### Trigger(s)\n\n#### Process Breakdown(s)\n\n#### Root Cause(s)\n\n### Mitigation & Resolution\n\n### Open Questions\n| Person            | Question/ Answer |\n|-------------------|------------------|\n| Q (who): A (who): |                  |\n| Q (who): A (who): |                  |\n| Q (who): A (who): |                  |\n\n### Lessons Learnt\n\n#### What went well\n\n#### What went badly\n\n#### Where did we get lucky\n\n### Action Items & Followups\n| Action Item | Type (Mitigate/ Prevent/ Process/ Other) | Who | Priority | Bug # | Due Date |\n|-------------|------------------------------------------|-----|----------|-------|----------|\n|             |                                          |     |          |       |          |\n|             |                                          |     |          |       |          |\n|             |                                          |     |          |       |          |\n\n### Supporting Documents\n\n\n"
          ],
          [
            "> Sample template from [Google Developers Blog: \"Google API infrastructure outage incident report\"](https://developers.googleblog.com/2013/05/google-api-infrastructure-outage_3.html).\n\n# Title\n\n## Issue Summary\n\n## Timeline\n> (all times in Pacific Time)\n\n## Root Cause\n\n## Resolution and recovery\n\n## Corrective and Preventative Measures\n\n\n\n# Hosted Graphite Postmortem Report (#2)\n\n## Outage\n**Period:** March 12th 2014 18:50 UTC to 19:20 (30 minutes)  \n**Impact:** Complete loss of new incoming data from all customers in the shared environment on all interfaces (UDP, TCP, TCP python pickle, HTTP)  \n**Additional Impact:** Significant frontend graph rendering latency, Frontend HTTP 500 errors\n\n## Timeline (all times are UTC, which is also local for the responding engineers)\n\n**18:46:** A user, still sending data to us, deletes their account.\n\n**18:50:** All 13 aggregation services attempt to remove some data for this user from a queue, but a lookup on the user's ID fails. Queue processing stops, and it starts building up in memory. No new metric data is written out.\n\n**18:56:** PagerDuty fires an alert and calls the on-call engineer.\n\n**18:56:** The on call engineer begins investigating.\n\n**19:00:** The first outage notification tweet is sent.\n\n**19:10:** After spending ten minutes investigating and attempting to recover the service, a decision is made to discard a large amount of data in memory in order to get the service back up and running quickly. The discarded data is copied in logs that can be replayed.\n\n**19:20:** The service is operating normally, but large chunks of recent data are missing from graphs.\n\n**19:30:** The on call engineer begins replaying a portion of the logs to restore the missing data, while a second engineer begins looking for the root cause. Normally we'd wait a bit before doing root cause analysis, but without a good understanding of the cause we felt it could happen again soon.\n\n**20:57:** Half of the restore has been completed.\n\n**22:00:** All restores complete. The only missing data is that which was lost during the actual outage, which lasted for 30 minutes.\n\n## Actions already taken\nWe have added better error handling to the aggregation services to fix this condition.\n\n## Plan\n1) While we were able to replay logs to restore some data, we didn't record enough to allow us to replay everything. We've had a project planned for a few months to record everything and hopefully allow complete recovery from outages, and we've changed some project priorities to get that work started next week.\n\n2) We still plan to publish an offsite status page to help keep customers informed during events like this.\n\n## Conclusion\nWe always regret losing data, it sucks and we're sorry we failed this time.\n\nBeing able to replay logs to recover from a failure is a pretty powerful tool. It helped make the decision to discard some data in order to recover the service faster, and we think that with more logging of incoming data that we'll be in a better position the next time we have an outage like this. \n\n> Sample template from: Thomas A. Limoncelli. [\u201cThe Practice of Cloud System Administration: Designing and Operating Large Distributed Systems, Volume 2.\u201d](http://the-cloud-book.com/).\n\n# Title\n\n### Report Status\n\n### Executive Summary\n\n### Outage Description\n\n### Affected users\n\n### Start Date/Time\n\n### End Date/Time\n\n### Duration\n\n## Timeline\n\n## Contributing Conditions Analysis\n\n### What went well?\n\n### What could have gone better?\n\n## Recommendations\n\n### Names of people involved\n\n\n\n> Template from: [Microsoft Azure status history](https://status.azure.com/en-us/status/history/).\n\n# Title (incident)\n\n### Date\n\n### Summary of impact\n\n### Customer impact\n\n### Root cause and mitigation\n\n### Next steps\n\n### Provide feedback (customer survey)\n\n\n# Shakespeare Sonnet++ Postmortem (incident #465)\n\n## Date\n\n2015-10-21\n\n## Authors\n\n* jennifer\n* martym\n* agoogler\n\n## Status\n\nComplete, action items in progress\n\n## Summary\n\nShakespeare Search down for 66 minutes during period of very high interest in Shakespeare due to discovery of a new sonnet.\n\n## Impact\n\nEstimated 1.21B queries lost, no revenue impact.\n\n## Root Causes\n\nCascading failure due to combination of exceptionally high load and a resource leak when searches failed due to terms not being in the Shakespeare corpus. The newly discovered sonnet used a word that had never before appeared in one of Shakespeare's works, which happened to be the term users searched for. Under normal circumstances, the rate of task failures due to resource leaks is low enough to be unnoticed.\n\n## Trigger\n\nLatent bug triggered by sudden increase in traffic.\n\n## Resolution\n\nDirected traffic to sacrificial cluster and added 10x capacity to mitigate cascading failure. Updated index deployed, resolving interaction with latent bug. Maintaining extra capacity until surge in public interest in new sonnet passes. Resource leak identified and fix deployed.\n\n## Detection\n\nBorgmon detected high level of HTTP 500s and paged on-call.\n\n## Action Items\n\n| Action Item | Type | Owner | Bug | Status |\n|-------------|------|-------|-----|--------|\n| Update playbook with instructions for responding to cascading failure | mitigate | jennifer | n/a | **DONE** |\n| Use flux capacitor to balance load between clusters | prevent | martym | Bug 5554823 | **TODO** |\n| Schedule cascading failure test during next DiRT | process | docbrown | n/a | **TODO** |\n| Investigate running index MR/fusion continuously | prevent | jennifer | Bug 5554824 | **TODO** |\n| Plug file descriptor leak in search ranking subsystem | prevent | agoogler | Bug 5554825 | **DONE** |\n| Add load shedding capabilities to Shakespeare search | prevent | agoogler | Bug 5554826 | **TODO** |\n| Build regression tests to ensure servers respond sanely to queries of death | prevent | clarac | Bug 5554827 | **TODO** |\n| Deploy updated search ranking subsystem to prod | prevent | jennifer | n/a | **DONE** |\n| Freeze production until 2015-11-20 due to error budget exhaustion | other | docbrown | n/a | **TODO** |\n\n## Lessons Learned\n\n### What went well\n\n* Monitoring quickly alerted us to high rate (reaching ~100%) of HTTP 500s\n* Rapidly distributed updated Shakespeare corpus to all clusters\n\n### What went wrong\n\n* We're out of practice in responding to cascading failure\n* We exceeded our availability error budget (by several orders of magnitude) due to the exceptional surge of traffic that essentially all resulted in failures\n\n### Where we got lucky\n\n* Mailing list of Shakespeare aficionados had a copy of new sonnet available\n* Server logs had stack traces pointing to file descriptor exhaustion as cause for crash\n* Query-of-death was resolved by pushing new index containing popular search term\n\n## Timeline\n\n2015-10-21 (*all times UTC*)\n\n| Time | Description |\n|------|-------------|\n| 14:51 | News reports that a new Shakespearean sonnet has been discovered in a Delorean's glove compartment |\n| 14:53 | Traffic to Shakespeare search increases by 88x after post to _/r/shakespeare_ points to Shakespeare search engine as place to find new sonnet (except we don't have the sonnet yet) |\n| 14:54 | **OUTAGE BEGINS** -- Search backends start melting down under load |\n| 14:55 | docbrown receives pager storm, ManyHttp500s from all clusters |\n| 14:57 | All traffic to Shakespeare search is failing |\n| 14:58 | docbrown starts investigating, finds backend crash rate very high |\n| 15:01 | **INCIDENT BEGINS** docbrown declares incident #465 due to cascading failure, coordination on #shakespeare, names jennifer incident commander |\n| 15:02 | someone coincidentally sends email to shakespeare-discuss@ re sonnet discovery, which happens to be at top of martym's inbox |\n| 15:03 | jennifer notifies shakespeare-incidents@ list of the incident |\n| 15:04 | martym tracks down text of new sonnet and looks for documentation on corpus update |\n| 15:06 | docbrown finds that crash symptoms identical across all tasks in all clusters, investigating cause based on application logs |\n| 15:07 | martym finds documentation, starts prep work for corpus update |\n| 15:10 | martym adds sonnet to Shakespeare's known works, starts indexing job |\n| 15:12 | docbrown contacts clarac & agoogler (from Shakespeare dev team) to help with examining codebase for possible causes |\n| 15:18 | clarac finds smoking gun in logs pointing to file descriptor exhaustion, confirms against code that leak exists if term not in corpus is searched for |\n| 15:20 | martym's index MapReduce job completes |\n| 15:21 | jennifer and docbrown decide to increase instance count enough to drop load on instances that they're able to do appreciable work before dying and being restarted |\n| 15:23 | docbrown load balances all traffic to USA-2 cluster, permitting instance count increase in other clusters without servers failing immediately |\n| 15:25 | martym starts replicating new index to all clusters |\n| 15:28 | docbrown starts 2x instance count increase |\n| 15:32 | jennifer changes load balancing to increase traffic to nonsacrificial clusters |\n| 15:33 | tasks in nonsacrificial clusters start failing, same symptoms as before |\n| 15:34 | found order-of-magnitude error in whiteboard calculations for instance count increase |\n| 15:36 | jennifer reverts load balancing to resacrifice USA-2 cluster in preparation for additional global 5x instance count increase (to a total of 10x initial capacity) |\n| 15:36 | **OUTAGE MITIGATED**, updated index replicated to all clusters |\n| 15:39 | docbrown starts second wave of instance count increase to 10x initial capacity |\n| 15:41 | jennifer reinstates load balancing across all clusters for 1% of traffic |\n| 15:43 | nonsacrificial clusters' HTTP 500 rates at nominal rates, task failures intermittent at low levels |\n| 15:45 | jennifer balances 10% of traffic across nonsacrificial clusters |\n| 15:47 | nonsacrificial clusters' HTTP 500 rates remain within SLO, no task failures observed |\n| 15:50 | 30% of traffic balanced across nonsacrificial clusters |\n| 15:55 | 50% of traffic balanced across nonsacrificial clusters |\n| 16:00 | **OUTAGE ENDS**, all traffic balanced across all clusters |\n| 16:30 | **INCIDENT ENDS**, reached exit criterion of 30 minutes' nominal performance |\n\n## Supporting Information\n\n* Monitoring dashboard, http://monitor/shakespeare?end%5Ftime=20151021T160000&duration=7200 \n\n> Template from: Betsy Beyer, Chris Jones, Jennifer Petoff, and Niall Richard Murphy. [\u201cSite Reliability Engineering.\u201d](https://landing.google.com/sre/book/chapters/postmortem.html).\n\n# Title (incident #)\n\n### Date\n\n### Authors\n\n### Status\n\n### Summary\n\n### Impact\n\n### Root Causes\n\n### Trigger\n\n### Resolution\n\n### Detection\n\n## Action Items\n\n## Lessons Learned\n\n### What went well\n\n### What went wrong\n\n### Where we got lucky\n\n## Timeline\n\n## Supporting information\n\n\n> Template from the [Elastic Cloud](https://www.elastic.co/blog/elastic-cloud-incident-report-feburary-4-2019) incident report.\n\n# Title of the incident: Date\n\nBy: (Names)\n\nBrief description\n\n## Background\n\n## What Happened\n\n### Chain of Events\n\n## Root Cause\n\n## Resolution\n\n## Impact\n\n## Action Items\n\n\n> Template from the [Real-world SRE](http://a.co/d/34UsUVU) book.\n\n# Title of the incident\n\nName(s):\n\nDate: \n\nLast modified: \n\n## Summary\n\n## Impact\n\n## Timeline\n\n## Root Cause(s)\n\n## Action Items\n\n## Appendix\n\n\n> Template from: [Michael Kehoe](https://michael-kehoe.io/post/postmortem-template/)\n\n# Postmortem Template\n\n### Summary\n| Incident Summary   |   |                       |   |\n|--------------------|---|-----------------------|---|\n| Incident Number    |   | Incident Severity     |   |\n| Postmortem Date    |   | War-room Required     |   |\n| SRE Lead           |   | Developer Lead        |   |\n| Incident Mgmt Lead |   | Chaos Eng Preventable |   |\n| Postmortem Lead    |   | Recording             |   |\n\n### Postmortem Attendees\n| Name | Role | In attendence |\n|------|------|---------------|\n|      |      |               |\n|      |      |               |\n|      |      |               |\n\n### Incident Timing\n| Start Time      |       | Incident Detected By(User-reported/ Ad-hoc monitoring/ Alerting system) |      |\n|-----------------|-------|-------------------------------------------------------------------------|------|\n| Detection Time  |       | Time to Detect (TTD)                                                    |      |\n| Mitigation Time |       | Time to Mitigate (TTM)                                                  |      |\n| Resolution Time |       | Time to Resolve (TTR)                                                   |      |\n\n### Incident Timeline\n| Date/Time | Who/What | Action/ Impact |\n|-----------|----------|----------------|\n|           |          |                |\n|           |          |                |\n|           |          |                |\n\n### Impact\n\n#### End User Impact\n\n#### Infrastructure Impact\n\n#### Productivity Impact\n\n### What caused the incident?\n\n#### Trigger(s)\n\n#### Process Breakdown(s)\n\n#### Root Cause(s)\n\n### Mitigation & Resolution\n\n### Open Questions\n| Person            | Question/ Answer |\n|-------------------|------------------|\n| Q (who): A (who): |                  |\n| Q (who): A (who): |                  |\n| Q (who): A (who): |                  |\n\n### Lessons Learnt\n\n#### What went well\n\n#### What went badly\n\n#### Where did we get lucky\n\n### Action Items & Followups\n| Action Item | Type (Mitigate/ Prevent/ Process/ Other) | Who | Priority | Bug # | Due Date |\n|-------------|------------------------------------------|-----|----------|-------|----------|\n|             |                                          |     |          |       |          |\n|             |                                          |     |          |       |          |\n|             |                                          |     |          |       |          |\n\n### Supporting Documents\n\n\n"
          ]
        ],
        "ground_truths": [
          "The root cause was a user account deletion while data was still being sent, causing aggregation services to fail when looking up the deleted user ID, which stopped queue processing and prevented new metric data from being written.",
          "The outage was resolved by discarding data in memory to quickly restore service, then replaying logs to restore missing data. The team added better error handling to prevent future occurrences."
        ],
        "answers": [
          "The root cause was a user account deletion while data was still being sent, causing aggregation services to fail when looking up the deleted user ID, which stopped queue processing and prevented new metric data from being written.",
          "The outage was resolved by discarding data in memory to quickly restore service, then replaying logs to restore missing data. The team added better error handling to prevent future occurrences."
        ]
      },
      "strategy": "naive"
    },
    "hybrid": {
      "scores": {
        "faithfulness": 0.875,
        "answer_relevancy": 0.68,
        "context_precision": 1.0,
        "context_recall": 1.0,
        "semantic_similarity": 1.0,
        "answer_correctness": 1.0
      },
      "evaluation_data": {
        "questions": [
          "What was the root cause of the Hosted Graphite outage on March 12th, 2014?",
          "How was the service outage resolved and what steps were taken?"
        ],
        "responses": [
          "The root cause of the Hosted Graphite outage on March 12th, 2014, was a failure in the lookup of a user's ID after the user deleted their account. This caused all 13 aggregation services to stop processing the queue, leading to a buildup in memory and a complete halt in writing new metric data.",
          "The service outage was resolved by discarding a large amount of data in memory to quickly get the service back up and running. The discarded data was copied into logs that could be replayed later. Once the service was operating normally, the on-call engineer began replaying a portion of the logs to restore the missing data. This process was completed by 22:00, with the only missing data being that which was lost during the actual 30-minute outage.\n\nSteps taken included:\n1. Discarding data in memory to restart the service.\n2. Copying the discarded data into logs for later replay.\n3. Replaying logs to restore missing data.\n4. Investigating the root cause to prevent future occurrences.\n5. Adding better error handling to the aggregation services.\n6. Planning to record all data to allow complete recovery from future outages.\n7. Planning to publish an offsite status page to keep customers informed during such events."
        ],
        "contexts": [
          [
            "# Hosted Graphite Postmortem Report (#2)\n\n## Outage\n**Period:** March 12th 2014 18:50 UTC to 19:20 (30 minutes)  \n**Impact:** Complete loss of new incoming data from all customers in the shared environment on all interfaces (UDP, TCP, TCP python pickle, HTTP)  \n**Additional Impact:** Significant frontend graph rendering latency, Frontend HTTP 500 errors\n\n## Timeline (all times are UTC, which is also local for the responding engineers)\n\n**18:46:** A user, still sending data to us, deletes their account.\n\n**18:50:** All 13 aggregation services attempt to remove some data for this user from a queue, but a lookup on the user's ID fails. Queue processing stops, and it starts building up in memory. No new metric data is written out.\n\n**18:56:** PagerDuty fires an alert and calls the on-call engineer.\n\n**18:56:** The on call engineer begins investigating.\n\n**19:00:** The first outage notification tweet is sent.\n\n**19:10:** After spending ten minutes investigating and attempting to recover the service, a decision is made to discard a large amount of data in memory in order to get the service back up and running quickly. The discarded data is copied in logs that can be replayed.\n\n**19:20:** The service is operating normally, but large chunks of recent data are missing from graphs.\n\n**19:30:** The on call engineer begins replaying a portion of the logs to restore the missing data, while a second engineer begins looking for the root cause. Normally we'd wait a bit before doing root cause analysis, but without a good understanding of the cause we felt it could happen again soon.\n\n**20:57:** Half of the restore has been completed.\n\n**22:00:** All restores complete. The only missing data is that which was lost during the actual outage, which lasted for 30 minutes.\n\n## Actions already taken\nWe have added better error handling to the aggregation services to fix this condition.\n\n## Plan\n1) While we were able to replay logs to restore some data, we didn't record enough to allow us to replay everything. We've had a project planned for a few months to record everything and hopefully allow complete recovery from outages, and we've changed some project priorities to get that work started next week.\n\n2) We still plan to publish an offsite status page to help keep customers informed during events like this.\n\n## Conclusion\nWe always regret losing data, it sucks and we're sorry we failed this time.\n\nBeing able to replay logs to recover from a failure is a pretty powerful tool. It helped make the decision to discard some data in order to recover the service faster, and we think that with more logging of incoming data that we'll be in a better position the next time we have an outage like this. \n\n> Sample template from [Google Developers Blog: \"Google API infrastructure outage incident report\"](https://developers.googleblog.com/2013/05/google-api-infrastructure-outage_3.html).\n\n# Title\n\n## Issue Summary\n\n## Timeline\n> (all times in Pacific Time)\n\n## Root Cause\n\n## Resolution and recovery\n\n## Corrective and Preventative Measures\n\n\n\n> Template from the [Elastic Cloud](https://www.elastic.co/blog/elastic-cloud-incident-report-feburary-4-2019) incident report.\n\n# Title of the incident: Date\n\nBy: (Names)\n\nBrief description\n\n## Background\n\n## What Happened\n\n### Chain of Events\n\n## Root Cause\n\n## Resolution\n\n## Impact\n\n## Action Items\n\n\n# Shakespeare Sonnet++ Postmortem (incident #465)\n\n## Date\n\n2015-10-21\n\n## Authors\n\n* jennifer\n* martym\n* agoogler\n\n## Status\n\nComplete, action items in progress\n\n## Summary\n\nShakespeare Search down for 66 minutes during period of very high interest in Shakespeare due to discovery of a new sonnet.\n\n## Impact\n\nEstimated 1.21B queries lost, no revenue impact.\n\n## Root Causes\n\nCascading failure due to combination of exceptionally high load and a resource leak when searches failed due to terms not being in the Shakespeare corpus. The newly discovered sonnet used a word that had never before appeared in one of Shakespeare's works, which happened to be the term users searched for. Under normal circumstances, the rate of task failures due to resource leaks is low enough to be unnoticed.\n\n## Trigger\n\nLatent bug triggered by sudden increase in traffic.\n\n## Resolution\n\nDirected traffic to sacrificial cluster and added 10x capacity to mitigate cascading failure. Updated index deployed, resolving interaction with latent bug. Maintaining extra capacity until surge in public interest in new sonnet passes. Resource leak identified and fix deployed.\n\n## Detection\n\nBorgmon detected high level of HTTP 500s and paged on-call.\n\n## Action Items\n\n| Action Item | Type | Owner | Bug | Status |\n|-------------|------|-------|-----|--------|\n| Update playbook with instructions for responding to cascading failure | mitigate | jennifer | n/a | **DONE** |\n| Use flux capacitor to balance load between clusters | prevent | martym | Bug 5554823 | **TODO** |\n| Schedule cascading failure test during next DiRT | process | docbrown | n/a | **TODO** |\n| Investigate running index MR/fusion continuously | prevent | jennifer | Bug 5554824 | **TODO** |\n| Plug file descriptor leak in search ranking subsystem | prevent | agoogler | Bug 5554825 | **DONE** |\n| Add load shedding capabilities to Shakespeare search | prevent | agoogler | Bug 5554826 | **TODO** |\n| Build regression tests to ensure servers respond sanely to queries of death | prevent | clarac | Bug 5554827 | **TODO** |\n| Deploy updated search ranking subsystem to prod | prevent | jennifer | n/a | **DONE** |\n| Freeze production until 2015-11-20 due to error budget exhaustion | other | docbrown | n/a | **TODO** |\n\n## Lessons Learned\n\n### What went well\n\n* Monitoring quickly alerted us to high rate (reaching ~100%) of HTTP 500s\n* Rapidly distributed updated Shakespeare corpus to all clusters\n\n### What went wrong\n\n* We're out of practice in responding to cascading failure\n* We exceeded our availability error budget (by several orders of magnitude) due to the exceptional surge of traffic that essentially all resulted in failures\n\n### Where we got lucky\n\n* Mailing list of Shakespeare aficionados had a copy of new sonnet available\n* Server logs had stack traces pointing to file descriptor exhaustion as cause for crash\n* Query-of-death was resolved by pushing new index containing popular search term\n\n## Timeline\n\n2015-10-21 (*all times UTC*)\n\n| Time | Description |\n|------|-------------|\n| 14:51 | News reports that a new Shakespearean sonnet has been discovered in a Delorean's glove compartment |\n| 14:53 | Traffic to Shakespeare search increases by 88x after post to _/r/shakespeare_ points to Shakespeare search engine as place to find new sonnet (except we don't have the sonnet yet) |\n| 14:54 | **OUTAGE BEGINS** -- Search backends start melting down under load |\n| 14:55 | docbrown receives pager storm, ManyHttp500s from all clusters |\n| 14:57 | All traffic to Shakespeare search is failing |\n| 14:58 | docbrown starts investigating, finds backend crash rate very high |\n| 15:01 | **INCIDENT BEGINS** docbrown declares incident #465 due to cascading failure, coordination on #shakespeare, names jennifer incident commander |\n| 15:02 | someone coincidentally sends email to shakespeare-discuss@ re sonnet discovery, which happens to be at top of martym's inbox |\n| 15:03 | jennifer notifies shakespeare-incidents@ list of the incident |\n| 15:04 | martym tracks down text of new sonnet and looks for documentation on corpus update |\n| 15:06 | docbrown finds that crash symptoms identical across all tasks in all clusters, investigating cause based on application logs |\n| 15:07 | martym finds documentation, starts prep work for corpus update |\n| 15:10 | martym adds sonnet to Shakespeare's known works, starts indexing job |\n| 15:12 | docbrown contacts clarac & agoogler (from Shakespeare dev team) to help with examining codebase for possible causes |\n| 15:18 | clarac finds smoking gun in logs pointing to file descriptor exhaustion, confirms against code that leak exists if term not in corpus is searched for |\n| 15:20 | martym's index MapReduce job completes |\n| 15:21 | jennifer and docbrown decide to increase instance count enough to drop load on instances that they're able to do appreciable work before dying and being restarted |\n| 15:23 | docbrown load balances all traffic to USA-2 cluster, permitting instance count increase in other clusters without servers failing immediately |\n| 15:25 | martym starts replicating new index to all clusters |\n| 15:28 | docbrown starts 2x instance count increase |\n| 15:32 | jennifer changes load balancing to increase traffic to nonsacrificial clusters |\n| 15:33 | tasks in nonsacrificial clusters start failing, same symptoms as before |\n| 15:34 | found order-of-magnitude error in whiteboard calculations for instance count increase |\n| 15:36 | jennifer reverts load balancing to resacrifice USA-2 cluster in preparation for additional global 5x instance count increase (to a total of 10x initial capacity) |\n| 15:36 | **OUTAGE MITIGATED**, updated index replicated to all clusters |\n| 15:39 | docbrown starts second wave of instance count increase to 10x initial capacity |\n| 15:41 | jennifer reinstates load balancing across all clusters for 1% of traffic |\n| 15:43 | nonsacrificial clusters' HTTP 500 rates at nominal rates, task failures intermittent at low levels |\n| 15:45 | jennifer balances 10% of traffic across nonsacrificial clusters |\n| 15:47 | nonsacrificial clusters' HTTP 500 rates remain within SLO, no task failures observed |\n| 15:50 | 30% of traffic balanced across nonsacrificial clusters |\n| 15:55 | 50% of traffic balanced across nonsacrificial clusters |\n| 16:00 | **OUTAGE ENDS**, all traffic balanced across all clusters |\n| 16:30 | **INCIDENT ENDS**, reached exit criterion of 30 minutes' nominal performance |\n\n## Supporting Information\n\n* Monitoring dashboard, http://monitor/shakespeare?end%5Ftime=20151021T160000&duration=7200 \n\n> Sample template from: Thomas A. Limoncelli. [\u201cThe Practice of Cloud System Administration: Designing and Operating Large Distributed Systems, Volume 2.\u201d](http://the-cloud-book.com/).\n\n# Title\n\n### Report Status\n\n### Executive Summary\n\n### Outage Description\n\n### Affected users\n\n### Start Date/Time\n\n### End Date/Time\n\n### Duration\n\n## Timeline\n\n## Contributing Conditions Analysis\n\n### What went well?\n\n### What could have gone better?\n\n## Recommendations\n\n### Names of people involved\n\n\n\n> Template from: Betsy Beyer, Chris Jones, Jennifer Petoff, and Niall Richard Murphy. [\u201cSite Reliability Engineering.\u201d](https://landing.google.com/sre/book/chapters/postmortem.html).\n\n# Title (incident #)\n\n### Date\n\n### Authors\n\n### Status\n\n### Summary\n\n### Impact\n\n### Root Causes\n\n### Trigger\n\n### Resolution\n\n### Detection\n\n## Action Items\n\n## Lessons Learned\n\n### What went well\n\n### What went wrong\n\n### Where we got lucky\n\n## Timeline\n\n## Supporting information\n\n\n> Template from: [Microsoft Azure status history](https://status.azure.com/en-us/status/history/).\n\n# Title (incident)\n\n### Date\n\n### Summary of impact\n\n### Customer impact\n\n### Root cause and mitigation\n\n### Next steps\n\n### Provide feedback (customer survey)\n\n\n> Template from the [Real-world SRE](http://a.co/d/34UsUVU) book.\n\n# Title of the incident\n\nName(s):\n\nDate: \n\nLast modified: \n\n## Summary\n\n## Impact\n\n## Timeline\n\n## Root Cause(s)\n\n## Action Items\n\n## Appendix\n\n\n> Template from: [Michael Kehoe](https://michael-kehoe.io/post/postmortem-template/)\n\n# Postmortem Template\n\n### Summary\n| Incident Summary   |   |                       |   |\n|--------------------|---|-----------------------|---|\n| Incident Number    |   | Incident Severity     |   |\n| Postmortem Date    |   | War-room Required     |   |\n| SRE Lead           |   | Developer Lead        |   |\n| Incident Mgmt Lead |   | Chaos Eng Preventable |   |\n| Postmortem Lead    |   | Recording             |   |\n\n### Postmortem Attendees\n| Name | Role | In attendence |\n|------|------|---------------|\n|      |      |               |\n|      |      |               |\n|      |      |               |\n\n### Incident Timing\n| Start Time      |       | Incident Detected By(User-reported/ Ad-hoc monitoring/ Alerting system) |      |\n|-----------------|-------|-------------------------------------------------------------------------|------|\n| Detection Time  |       | Time to Detect (TTD)                                                    |      |\n| Mitigation Time |       | Time to Mitigate (TTM)                                                  |      |\n| Resolution Time |       | Time to Resolve (TTR)                                                   |      |\n\n### Incident Timeline\n| Date/Time | Who/What | Action/ Impact |\n|-----------|----------|----------------|\n|           |          |                |\n|           |          |                |\n|           |          |                |\n\n### Impact\n\n#### End User Impact\n\n#### Infrastructure Impact\n\n#### Productivity Impact\n\n### What caused the incident?\n\n#### Trigger(s)\n\n#### Process Breakdown(s)\n\n#### Root Cause(s)\n\n### Mitigation & Resolution\n\n### Open Questions\n| Person            | Question/ Answer |\n|-------------------|------------------|\n| Q (who): A (who): |                  |\n| Q (who): A (who): |                  |\n| Q (who): A (who): |                  |\n\n### Lessons Learnt\n\n#### What went well\n\n#### What went badly\n\n#### Where did we get lucky\n\n### Action Items & Followups\n| Action Item | Type (Mitigate/ Prevent/ Process/ Other) | Who | Priority | Bug # | Due Date |\n|-------------|------------------------------------------|-----|----------|-------|----------|\n|             |                                          |     |          |       |          |\n|             |                                          |     |          |       |          |\n|             |                                          |     |          |       |          |\n\n### Supporting Documents\n\n\n"
          ],
          [
            "> Sample template from [Google Developers Blog: \"Google API infrastructure outage incident report\"](https://developers.googleblog.com/2013/05/google-api-infrastructure-outage_3.html).\n\n# Title\n\n## Issue Summary\n\n## Timeline\n> (all times in Pacific Time)\n\n## Root Cause\n\n## Resolution and recovery\n\n## Corrective and Preventative Measures\n\n\n\n# Hosted Graphite Postmortem Report (#2)\n\n## Outage\n**Period:** March 12th 2014 18:50 UTC to 19:20 (30 minutes)  \n**Impact:** Complete loss of new incoming data from all customers in the shared environment on all interfaces (UDP, TCP, TCP python pickle, HTTP)  \n**Additional Impact:** Significant frontend graph rendering latency, Frontend HTTP 500 errors\n\n## Timeline (all times are UTC, which is also local for the responding engineers)\n\n**18:46:** A user, still sending data to us, deletes their account.\n\n**18:50:** All 13 aggregation services attempt to remove some data for this user from a queue, but a lookup on the user's ID fails. Queue processing stops, and it starts building up in memory. No new metric data is written out.\n\n**18:56:** PagerDuty fires an alert and calls the on-call engineer.\n\n**18:56:** The on call engineer begins investigating.\n\n**19:00:** The first outage notification tweet is sent.\n\n**19:10:** After spending ten minutes investigating and attempting to recover the service, a decision is made to discard a large amount of data in memory in order to get the service back up and running quickly. The discarded data is copied in logs that can be replayed.\n\n**19:20:** The service is operating normally, but large chunks of recent data are missing from graphs.\n\n**19:30:** The on call engineer begins replaying a portion of the logs to restore the missing data, while a second engineer begins looking for the root cause. Normally we'd wait a bit before doing root cause analysis, but without a good understanding of the cause we felt it could happen again soon.\n\n**20:57:** Half of the restore has been completed.\n\n**22:00:** All restores complete. The only missing data is that which was lost during the actual outage, which lasted for 30 minutes.\n\n## Actions already taken\nWe have added better error handling to the aggregation services to fix this condition.\n\n## Plan\n1) While we were able to replay logs to restore some data, we didn't record enough to allow us to replay everything. We've had a project planned for a few months to record everything and hopefully allow complete recovery from outages, and we've changed some project priorities to get that work started next week.\n\n2) We still plan to publish an offsite status page to help keep customers informed during events like this.\n\n## Conclusion\nWe always regret losing data, it sucks and we're sorry we failed this time.\n\nBeing able to replay logs to recover from a failure is a pretty powerful tool. It helped make the decision to discard some data in order to recover the service faster, and we think that with more logging of incoming data that we'll be in a better position the next time we have an outage like this. \n\n> Template from: [Microsoft Azure status history](https://status.azure.com/en-us/status/history/).\n\n# Title (incident)\n\n### Date\n\n### Summary of impact\n\n### Customer impact\n\n### Root cause and mitigation\n\n### Next steps\n\n### Provide feedback (customer survey)\n\n\n> Sample template from: Thomas A. Limoncelli. [\u201cThe Practice of Cloud System Administration: Designing and Operating Large Distributed Systems, Volume 2.\u201d](http://the-cloud-book.com/).\n\n# Title\n\n### Report Status\n\n### Executive Summary\n\n### Outage Description\n\n### Affected users\n\n### Start Date/Time\n\n### End Date/Time\n\n### Duration\n\n## Timeline\n\n## Contributing Conditions Analysis\n\n### What went well?\n\n### What could have gone better?\n\n## Recommendations\n\n### Names of people involved\n\n\n\n# Shakespeare Sonnet++ Postmortem (incident #465)\n\n## Date\n\n2015-10-21\n\n## Authors\n\n* jennifer\n* martym\n* agoogler\n\n## Status\n\nComplete, action items in progress\n\n## Summary\n\nShakespeare Search down for 66 minutes during period of very high interest in Shakespeare due to discovery of a new sonnet.\n\n## Impact\n\nEstimated 1.21B queries lost, no revenue impact.\n\n## Root Causes\n\nCascading failure due to combination of exceptionally high load and a resource leak when searches failed due to terms not being in the Shakespeare corpus. The newly discovered sonnet used a word that had never before appeared in one of Shakespeare's works, which happened to be the term users searched for. Under normal circumstances, the rate of task failures due to resource leaks is low enough to be unnoticed.\n\n## Trigger\n\nLatent bug triggered by sudden increase in traffic.\n\n## Resolution\n\nDirected traffic to sacrificial cluster and added 10x capacity to mitigate cascading failure. Updated index deployed, resolving interaction with latent bug. Maintaining extra capacity until surge in public interest in new sonnet passes. Resource leak identified and fix deployed.\n\n## Detection\n\nBorgmon detected high level of HTTP 500s and paged on-call.\n\n## Action Items\n\n| Action Item | Type | Owner | Bug | Status |\n|-------------|------|-------|-----|--------|\n| Update playbook with instructions for responding to cascading failure | mitigate | jennifer | n/a | **DONE** |\n| Use flux capacitor to balance load between clusters | prevent | martym | Bug 5554823 | **TODO** |\n| Schedule cascading failure test during next DiRT | process | docbrown | n/a | **TODO** |\n| Investigate running index MR/fusion continuously | prevent | jennifer | Bug 5554824 | **TODO** |\n| Plug file descriptor leak in search ranking subsystem | prevent | agoogler | Bug 5554825 | **DONE** |\n| Add load shedding capabilities to Shakespeare search | prevent | agoogler | Bug 5554826 | **TODO** |\n| Build regression tests to ensure servers respond sanely to queries of death | prevent | clarac | Bug 5554827 | **TODO** |\n| Deploy updated search ranking subsystem to prod | prevent | jennifer | n/a | **DONE** |\n| Freeze production until 2015-11-20 due to error budget exhaustion | other | docbrown | n/a | **TODO** |\n\n## Lessons Learned\n\n### What went well\n\n* Monitoring quickly alerted us to high rate (reaching ~100%) of HTTP 500s\n* Rapidly distributed updated Shakespeare corpus to all clusters\n\n### What went wrong\n\n* We're out of practice in responding to cascading failure\n* We exceeded our availability error budget (by several orders of magnitude) due to the exceptional surge of traffic that essentially all resulted in failures\n\n### Where we got lucky\n\n* Mailing list of Shakespeare aficionados had a copy of new sonnet available\n* Server logs had stack traces pointing to file descriptor exhaustion as cause for crash\n* Query-of-death was resolved by pushing new index containing popular search term\n\n## Timeline\n\n2015-10-21 (*all times UTC*)\n\n| Time | Description |\n|------|-------------|\n| 14:51 | News reports that a new Shakespearean sonnet has been discovered in a Delorean's glove compartment |\n| 14:53 | Traffic to Shakespeare search increases by 88x after post to _/r/shakespeare_ points to Shakespeare search engine as place to find new sonnet (except we don't have the sonnet yet) |\n| 14:54 | **OUTAGE BEGINS** -- Search backends start melting down under load |\n| 14:55 | docbrown receives pager storm, ManyHttp500s from all clusters |\n| 14:57 | All traffic to Shakespeare search is failing |\n| 14:58 | docbrown starts investigating, finds backend crash rate very high |\n| 15:01 | **INCIDENT BEGINS** docbrown declares incident #465 due to cascading failure, coordination on #shakespeare, names jennifer incident commander |\n| 15:02 | someone coincidentally sends email to shakespeare-discuss@ re sonnet discovery, which happens to be at top of martym's inbox |\n| 15:03 | jennifer notifies shakespeare-incidents@ list of the incident |\n| 15:04 | martym tracks down text of new sonnet and looks for documentation on corpus update |\n| 15:06 | docbrown finds that crash symptoms identical across all tasks in all clusters, investigating cause based on application logs |\n| 15:07 | martym finds documentation, starts prep work for corpus update |\n| 15:10 | martym adds sonnet to Shakespeare's known works, starts indexing job |\n| 15:12 | docbrown contacts clarac & agoogler (from Shakespeare dev team) to help with examining codebase for possible causes |\n| 15:18 | clarac finds smoking gun in logs pointing to file descriptor exhaustion, confirms against code that leak exists if term not in corpus is searched for |\n| 15:20 | martym's index MapReduce job completes |\n| 15:21 | jennifer and docbrown decide to increase instance count enough to drop load on instances that they're able to do appreciable work before dying and being restarted |\n| 15:23 | docbrown load balances all traffic to USA-2 cluster, permitting instance count increase in other clusters without servers failing immediately |\n| 15:25 | martym starts replicating new index to all clusters |\n| 15:28 | docbrown starts 2x instance count increase |\n| 15:32 | jennifer changes load balancing to increase traffic to nonsacrificial clusters |\n| 15:33 | tasks in nonsacrificial clusters start failing, same symptoms as before |\n| 15:34 | found order-of-magnitude error in whiteboard calculations for instance count increase |\n| 15:36 | jennifer reverts load balancing to resacrifice USA-2 cluster in preparation for additional global 5x instance count increase (to a total of 10x initial capacity) |\n| 15:36 | **OUTAGE MITIGATED**, updated index replicated to all clusters |\n| 15:39 | docbrown starts second wave of instance count increase to 10x initial capacity |\n| 15:41 | jennifer reinstates load balancing across all clusters for 1% of traffic |\n| 15:43 | nonsacrificial clusters' HTTP 500 rates at nominal rates, task failures intermittent at low levels |\n| 15:45 | jennifer balances 10% of traffic across nonsacrificial clusters |\n| 15:47 | nonsacrificial clusters' HTTP 500 rates remain within SLO, no task failures observed |\n| 15:50 | 30% of traffic balanced across nonsacrificial clusters |\n| 15:55 | 50% of traffic balanced across nonsacrificial clusters |\n| 16:00 | **OUTAGE ENDS**, all traffic balanced across all clusters |\n| 16:30 | **INCIDENT ENDS**, reached exit criterion of 30 minutes' nominal performance |\n\n## Supporting Information\n\n* Monitoring dashboard, http://monitor/shakespeare?end%5Ftime=20151021T160000&duration=7200 \n\n> Template from: Betsy Beyer, Chris Jones, Jennifer Petoff, and Niall Richard Murphy. [\u201cSite Reliability Engineering.\u201d](https://landing.google.com/sre/book/chapters/postmortem.html).\n\n# Title (incident #)\n\n### Date\n\n### Authors\n\n### Status\n\n### Summary\n\n### Impact\n\n### Root Causes\n\n### Trigger\n\n### Resolution\n\n### Detection\n\n## Action Items\n\n## Lessons Learned\n\n### What went well\n\n### What went wrong\n\n### Where we got lucky\n\n## Timeline\n\n## Supporting information\n\n\n> Template from the [Real-world SRE](http://a.co/d/34UsUVU) book.\n\n# Title of the incident\n\nName(s):\n\nDate: \n\nLast modified: \n\n## Summary\n\n## Impact\n\n## Timeline\n\n## Root Cause(s)\n\n## Action Items\n\n## Appendix\n\n\n> Template from the [Elastic Cloud](https://www.elastic.co/blog/elastic-cloud-incident-report-feburary-4-2019) incident report.\n\n# Title of the incident: Date\n\nBy: (Names)\n\nBrief description\n\n## Background\n\n## What Happened\n\n### Chain of Events\n\n## Root Cause\n\n## Resolution\n\n## Impact\n\n## Action Items\n\n\n> Template from: [Michael Kehoe](https://michael-kehoe.io/post/postmortem-template/)\n\n# Postmortem Template\n\n### Summary\n| Incident Summary   |   |                       |   |\n|--------------------|---|-----------------------|---|\n| Incident Number    |   | Incident Severity     |   |\n| Postmortem Date    |   | War-room Required     |   |\n| SRE Lead           |   | Developer Lead        |   |\n| Incident Mgmt Lead |   | Chaos Eng Preventable |   |\n| Postmortem Lead    |   | Recording             |   |\n\n### Postmortem Attendees\n| Name | Role | In attendence |\n|------|------|---------------|\n|      |      |               |\n|      |      |               |\n|      |      |               |\n\n### Incident Timing\n| Start Time      |       | Incident Detected By(User-reported/ Ad-hoc monitoring/ Alerting system) |      |\n|-----------------|-------|-------------------------------------------------------------------------|------|\n| Detection Time  |       | Time to Detect (TTD)                                                    |      |\n| Mitigation Time |       | Time to Mitigate (TTM)                                                  |      |\n| Resolution Time |       | Time to Resolve (TTR)                                                   |      |\n\n### Incident Timeline\n| Date/Time | Who/What | Action/ Impact |\n|-----------|----------|----------------|\n|           |          |                |\n|           |          |                |\n|           |          |                |\n\n### Impact\n\n#### End User Impact\n\n#### Infrastructure Impact\n\n#### Productivity Impact\n\n### What caused the incident?\n\n#### Trigger(s)\n\n#### Process Breakdown(s)\n\n#### Root Cause(s)\n\n### Mitigation & Resolution\n\n### Open Questions\n| Person            | Question/ Answer |\n|-------------------|------------------|\n| Q (who): A (who): |                  |\n| Q (who): A (who): |                  |\n| Q (who): A (who): |                  |\n\n### Lessons Learnt\n\n#### What went well\n\n#### What went badly\n\n#### Where did we get lucky\n\n### Action Items & Followups\n| Action Item | Type (Mitigate/ Prevent/ Process/ Other) | Who | Priority | Bug # | Due Date |\n|-------------|------------------------------------------|-----|----------|-------|----------|\n|             |                                          |     |          |       |          |\n|             |                                          |     |          |       |          |\n|             |                                          |     |          |       |          |\n\n### Supporting Documents\n\n\n"
          ]
        ],
        "ground_truths": [
          "The root cause was a user account deletion while data was still being sent, causing aggregation services to fail when looking up the deleted user ID, which stopped queue processing and prevented new metric data from being written.",
          "The outage was resolved by discarding data in memory to quickly restore service, then replaying logs to restore missing data. The team added better error handling to prevent future occurrences."
        ],
        "answers": [
          "The root cause was a user account deletion while data was still being sent, causing aggregation services to fail when looking up the deleted user ID, which stopped queue processing and prevented new metric data from being written.",
          "The outage was resolved by discarding data in memory to quickly restore service, then replaying logs to restore missing data. The team added better error handling to prevent future occurrences."
        ]
      },
      "strategy": "hybrid"
    },
    "ensemble": {
      "scores": {
        "faithfulness": 0.0,
        "answer_relevancy": 0.68,
        "context_precision": 0.0,
        "context_recall": 0.0,
        "semantic_similarity": 1.0,
        "answer_correctness": 0.906
      },
      "evaluation_data": {
        "questions": [
          "What was the root cause of the Hosted Graphite outage on March 12th, 2014?",
          "How was the service outage resolved and what steps were taken?"
        ],
        "responses": [
          "Error generating response",
          "Error generating response"
        ],
        "contexts": [
          [],
          []
        ],
        "ground_truths": [
          "The root cause was a user account deletion while data was still being sent, causing aggregation services to fail when looking up the deleted user ID, which stopped queue processing and prevented new metric data from being written.",
          "The outage was resolved by discarding data in memory to quickly restore service, then replaying logs to restore missing data. The team added better error handling to prevent future occurrences."
        ],
        "answers": [
          "The root cause was a user account deletion while data was still being sent, causing aggregation services to fail when looking up the deleted user ID, which stopped queue processing and prevented new metric data from being written.",
          "The outage was resolved by discarding data in memory to quickly restore service, then replaying logs to restore missing data. The team added better error handling to prevent future occurrences."
        ]
      },
      "strategy": "ensemble"
    }
  },
  "improvements": {
    "naive": {
      "faithfulness": {
        "baseline": 0.267,
        "current": 1.0,
        "improvement": 0.733,
        "improvement_pct": 274.5318352059925
      },
      "answer_relevancy": {
        "baseline": 0.518,
        "current": 0.68,
        "improvement": 0.16200000000000003,
        "improvement_pct": 31.27413127413128
      },
      "context_precision": {
        "baseline": 0.75,
        "current": 1.0,
        "improvement": 0.25,
        "improvement_pct": 33.33333333333333
      },
      "context_recall": {
        "baseline": 0.833,
        "current": 1.0,
        "improvement": 0.16700000000000004,
        "improvement_pct": 20.04801920768308
      },
      "semantic_similarity": {
        "baseline": 0.437,
        "current": 1.0,
        "improvement": 0.563,
        "improvement_pct": 128.83295194508008
      },
      "answer_correctness": {
        "baseline": 0.163,
        "current": 1.0,
        "improvement": 0.837,
        "improvement_pct": 513.4969325153373
      }
    },
    "hybrid": {
      "faithfulness": {
        "baseline": 0.267,
        "current": 0.875,
        "improvement": 0.608,
        "improvement_pct": 227.71535580524343
      },
      "answer_relevancy": {
        "baseline": 0.518,
        "current": 0.68,
        "improvement": 0.16200000000000003,
        "improvement_pct": 31.27413127413128
      },
      "context_precision": {
        "baseline": 0.75,
        "current": 1.0,
        "improvement": 0.25,
        "improvement_pct": 33.33333333333333
      },
      "context_recall": {
        "baseline": 0.833,
        "current": 1.0,
        "improvement": 0.16700000000000004,
        "improvement_pct": 20.04801920768308
      },
      "semantic_similarity": {
        "baseline": 0.437,
        "current": 1.0,
        "improvement": 0.563,
        "improvement_pct": 128.83295194508008
      },
      "answer_correctness": {
        "baseline": 0.163,
        "current": 1.0,
        "improvement": 0.837,
        "improvement_pct": 513.4969325153373
      }
    },
    "ensemble": {
      "faithfulness": {
        "baseline": 0.267,
        "current": 0.0,
        "improvement": -0.267,
        "improvement_pct": -100.0
      },
      "answer_relevancy": {
        "baseline": 0.518,
        "current": 0.68,
        "improvement": 0.16200000000000003,
        "improvement_pct": 31.27413127413128
      },
      "context_precision": {
        "baseline": 0.75,
        "current": 0.0,
        "improvement": -0.75,
        "improvement_pct": -100.0
      },
      "context_recall": {
        "baseline": 0.833,
        "current": 0.0,
        "improvement": -0.833,
        "improvement_pct": -100.0
      },
      "semantic_similarity": {
        "baseline": 0.437,
        "current": 1.0,
        "improvement": 0.563,
        "improvement_pct": 128.83295194508008
      },
      "answer_correctness": {
        "baseline": 0.163,
        "current": 0.906,
        "improvement": 0.743,
        "improvement_pct": 455.8282208588957
      }
    }
  }
}