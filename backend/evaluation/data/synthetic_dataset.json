{
  "metadata": {
    "generated_at": "2025-08-04T14:06:27.805238",
    "total_questions": 30,
    "source_postmortems": 3,
    "categories": [
      "root_cause",
      "resolution",
      "impact",
      "prevention",
      "detection"
    ],
    "generator_version": "1.0.0"
  },
  "questions": [
    "What was the root cause of the complete loss of new incoming data during the Hosted Graphite outage on March 12th, 2014?",
    "What was the root cause of the queue processing failure that led to the service outage?",
    "What actions were taken to resolve the system failure and restore service after the outage on March 12th, 2014?",
    "What immediate action did the on-call engineer take to resolve the service issue during the outage?",
    "What was the impact of the outage on March 12th, 2014, for customers using the Hosted Graphite service?",
    "What was the impact on data availability during the outage on March 12th, 2014?",
    "What measures could have been implemented to prevent the aggregation services from failing to process the queue when a user deletes their account?",
    "What preventive measures were recommended to avoid a similar incident where data processing stops due to a failed user ID lookup?",
    "How was the system failure incident on March 12th, 2014 detected by the on-call engineers?",
    "What alert was triggered that notified the on-call engineer of the outage?",
    "What specific misconfiguration in the database caused the service outage on September 15th?",
    "What was the root cause of the service outage experienced on September 15th?",
    "What steps were taken to resolve the database connection timeout issue during the incident?",
    "What steps were taken to resolve the service outage caused by the database misconfiguration?",
    "What was the impact on customer transactions during the system failure incident?",
    "How many users experienced disruptions during the service outage on March 5th?",
    "What measures could have been implemented to prevent the database outage caused by the unexpected surge in traffic?",
    "What preventive measures were recommended to avoid future service outages due to the database connection pool exhaustion?",
    "How was the system outage on October 15th detected?",
    "What alerts fired during the service outage on September 15th that helped detect the incident?",
    "What was the root cause of the Shakespeare Search system failure during the incident on 2015-10-21?",
    "What was the root cause of the service failure during the incident on 2015-10-21?",
    "What steps were taken to resolve the cascading failure caused by the high load and resource leak during the Shakespeare Search incident?",
    "What steps were taken to resolve the cascading failure during the Shakespeare Sonnet++ incident?",
    "What was the impact of the Shakespeare Search outage during the incident on October 21, 2015?",
    "How many queries were lost during the Shakespeare Search outage?",
    "What measures could have been implemented to prevent the cascading failure caused by the sudden surge in traffic and the resource leak?",
    "What preventive measures were recommended to avoid a similar incident in the future following the Shakespeare Search outage?",
    "How was the system failure incident detected during the Shakespeare Sonnet++ incident on 2015-10-21?",
    "What alerts fired during the service outage?"
  ],
  "contexts": [
    [
      "# Hosted Graphite Postmortem Report (#2)\n\n## Outage\n**Period:** March 12th 2014 18:50 UTC to 19:20 (30 minutes)  \n**Impact:** Complete loss of new incoming data from all customers in the shared environment on all interfaces (UDP, TCP, TCP python pickle, HTTP)  \n**Additional Impact:** Significant frontend graph rendering latency, Frontend HTTP 500 errors\n\n## Timeline (all times are UTC, which is also local for the responding engineers)\n\n**18:46:** A user, still sending data to us, deletes their account.\n\n**18:50:** All 13 aggregation services attempt to remove some data for this user from a queue, but a lookup on the user's ID fails. Queue processing stops, and it starts building up in memory. No new metric data is written out.\n\n**18:56:** PagerDuty fires an alert and calls the on-call engineer.\n\n**18:56:** The on call engineer begins investigating.\n\n**19:00:** The first outage notification tweet is sent.\n\n**19:10:** After spending ten minutes investigating and attempting to recover the service, a decision is made to discard a large amount of data in memory in order to get the service back up and running quickly. The discarded data is copied in logs that can be replayed.\n\n**19:20:** The service is operating normally, but large chunks of recent data are missing from graphs.\n\n**19:30:** The on call engineer begins replaying a portion of the logs to restore the missing data, while a second engineer begins looking for the root cause. Normally we'd wait a bit before doing root cause analysis, but without a good understanding of the cause we felt it could happen again soon.\n\n**20:57:** Half of the restore has been completed.\n\n**22:00:** All restores complete. The only missing data is that which was lost during the actual outage, which lasted for 30 minutes.\n\n## Actions already taken\nWe have added better error handling to the aggregation services to fix this condition.\n\n## Plan\n1) While we were able to replay logs to restore some data, we didn't record enough to allow us to"
    ],
    [
      "# Hosted Graphite Postmortem Report (#2)\n\n## Outage\n**Period:** March 12th 2014 18:50 UTC to 19:20 (30 minutes)  \n**Impact:** Complete loss of new incoming data from all customers in the shared environment on all interfaces (UDP, TCP, TCP python pickle, HTTP)  \n**Additional Impact:** Significant frontend graph rendering latency, Frontend HTTP 500 errors\n\n## Timeline (all times are UTC, which is also local for the responding engineers)\n\n**18:46:** A user, still sending data to us, deletes their account.\n\n**18:50:** All 13 aggregation services attempt to remove some data for this user from a queue, but a lookup on the user's ID fails. Queue processing stops, and it starts building up in memory. No new metric data is written out.\n\n**18:56:** PagerDuty fires an alert and calls the on-call engineer.\n\n**18:56:** The on call engineer begins investigating.\n\n**19:00:** The first outage notification tweet is sent.\n\n**19:10:** After spending ten minutes investigating and attempting to recover the service, a decision is made to discard a large amount of data in memory in order to get the service back up and running quickly. The discarded data is copied in logs that can be replayed.\n\n**19:20:** The service is operating normally, but large chunks of recent data are missing from graphs.\n\n**19:30:** The on call engineer begins replaying a portion of the logs to restore the missing data, while a second engineer begins looking for the root cause. Normally we'd wait a bit before doing root cause analysis, but without a good understanding of the cause we felt it could happen again soon.\n\n**20:57:** Half of the restore has been completed.\n\n**22:00:** All restores complete. The only missing data is that which was lost during the actual outage, which lasted for 30 minutes.\n\n## Actions already taken\nWe have added better error handling to the aggregation services to fix this condition.\n\n## Plan\n1) While we were able to replay logs to restore some data, we didn't record enough to allow us to"
    ],
    [
      "# Hosted Graphite Postmortem Report (#2)\n\n## Outage\n**Period:** March 12th 2014 18:50 UTC to 19:20 (30 minutes)  \n**Impact:** Complete loss of new incoming data from all customers in the shared environment on all interfaces (UDP, TCP, TCP python pickle, HTTP)  \n**Additional Impact:** Significant frontend graph rendering latency, Frontend HTTP 500 errors\n\n## Timeline (all times are UTC, which is also local for the responding engineers)\n\n**18:46:** A user, still sending data to us, deletes their account.\n\n**18:50:** All 13 aggregation services attempt to remove some data for this user from a queue, but a lookup on the user's ID fails. Queue processing stops, and it starts building up in memory. No new metric data is written out.\n\n**18:56:** PagerDuty fires an alert and calls the on-call engineer.\n\n**18:56:** The on call engineer begins investigating.\n\n**19:00:** The first outage notification tweet is sent.\n\n**19:10:** After spending ten minutes investigating and attempting to recover the service, a decision is made to discard a large amount of data in memory in order to get the service back up and running quickly. The discarded data is copied in logs that can be replayed.\n\n**19:20:** The service is operating normally, but large chunks of recent data are missing from graphs.\n\n**19:30:** The on call engineer begins replaying a portion of the logs to restore the missing data, while a second engineer begins looking for the root cause. Normally we'd wait a bit before doing root cause analysis, but without a good understanding of the cause we felt it could happen again soon.\n\n**20:57:** Half of the restore has been completed.\n\n**22:00:** All restores complete. The only missing data is that which was lost during the actual outage, which lasted for 30 minutes.\n\n## Actions already taken\nWe have added better error handling to the aggregation services to fix this condition.\n\n## Plan\n1) While we were able to replay logs to restore some data, we didn't record enough to allow us to"
    ],
    [
      "# Hosted Graphite Postmortem Report (#2)\n\n## Outage\n**Period:** March 12th 2014 18:50 UTC to 19:20 (30 minutes)  \n**Impact:** Complete loss of new incoming data from all customers in the shared environment on all interfaces (UDP, TCP, TCP python pickle, HTTP)  \n**Additional Impact:** Significant frontend graph rendering latency, Frontend HTTP 500 errors\n\n## Timeline (all times are UTC, which is also local for the responding engineers)\n\n**18:46:** A user, still sending data to us, deletes their account.\n\n**18:50:** All 13 aggregation services attempt to remove some data for this user from a queue, but a lookup on the user's ID fails. Queue processing stops, and it starts building up in memory. No new metric data is written out.\n\n**18:56:** PagerDuty fires an alert and calls the on-call engineer.\n\n**18:56:** The on call engineer begins investigating.\n\n**19:00:** The first outage notification tweet is sent.\n\n**19:10:** After spending ten minutes investigating and attempting to recover the service, a decision is made to discard a large amount of data in memory in order to get the service back up and running quickly. The discarded data is copied in logs that can be replayed.\n\n**19:20:** The service is operating normally, but large chunks of recent data are missing from graphs.\n\n**19:30:** The on call engineer begins replaying a portion of the logs to restore the missing data, while a second engineer begins looking for the root cause. Normally we'd wait a bit before doing root cause analysis, but without a good understanding of the cause we felt it could happen again soon.\n\n**20:57:** Half of the restore has been completed.\n\n**22:00:** All restores complete. The only missing data is that which was lost during the actual outage, which lasted for 30 minutes.\n\n## Actions already taken\nWe have added better error handling to the aggregation services to fix this condition.\n\n## Plan\n1) While we were able to replay logs to restore some data, we didn't record enough to allow us to"
    ],
    [
      "# Hosted Graphite Postmortem Report (#2)\n\n## Outage\n**Period:** March 12th 2014 18:50 UTC to 19:20 (30 minutes)  \n**Impact:** Complete loss of new incoming data from all customers in the shared environment on all interfaces (UDP, TCP, TCP python pickle, HTTP)  \n**Additional Impact:** Significant frontend graph rendering latency, Frontend HTTP 500 errors\n\n## Timeline (all times are UTC, which is also local for the responding engineers)\n\n**18:46:** A user, still sending data to us, deletes their account.\n\n**18:50:** All 13 aggregation services attempt to remove some data for this user from a queue, but a lookup on the user's ID fails. Queue processing stops, and it starts building up in memory. No new metric data is written out.\n\n**18:56:** PagerDuty fires an alert and calls the on-call engineer.\n\n**18:56:** The on call engineer begins investigating.\n\n**19:00:** The first outage notification tweet is sent.\n\n**19:10:** After spending ten minutes investigating and attempting to recover the service, a decision is made to discard a large amount of data in memory in order to get the service back up and running quickly. The discarded data is copied in logs that can be replayed.\n\n**19:20:** The service is operating normally, but large chunks of recent data are missing from graphs.\n\n**19:30:** The on call engineer begins replaying a portion of the logs to restore the missing data, while a second engineer begins looking for the root cause. Normally we'd wait a bit before doing root cause analysis, but without a good understanding of the cause we felt it could happen again soon.\n\n**20:57:** Half of the restore has been completed.\n\n**22:00:** All restores complete. The only missing data is that which was lost during the actual outage, which lasted for 30 minutes.\n\n## Actions already taken\nWe have added better error handling to the aggregation services to fix this condition.\n\n## Plan\n1) While we were able to replay logs to restore some data, we didn't record enough to allow us to"
    ],
    [
      "# Hosted Graphite Postmortem Report (#2)\n\n## Outage\n**Period:** March 12th 2014 18:50 UTC to 19:20 (30 minutes)  \n**Impact:** Complete loss of new incoming data from all customers in the shared environment on all interfaces (UDP, TCP, TCP python pickle, HTTP)  \n**Additional Impact:** Significant frontend graph rendering latency, Frontend HTTP 500 errors\n\n## Timeline (all times are UTC, which is also local for the responding engineers)\n\n**18:46:** A user, still sending data to us, deletes their account.\n\n**18:50:** All 13 aggregation services attempt to remove some data for this user from a queue, but a lookup on the user's ID fails. Queue processing stops, and it starts building up in memory. No new metric data is written out.\n\n**18:56:** PagerDuty fires an alert and calls the on-call engineer.\n\n**18:56:** The on call engineer begins investigating.\n\n**19:00:** The first outage notification tweet is sent.\n\n**19:10:** After spending ten minutes investigating and attempting to recover the service, a decision is made to discard a large amount of data in memory in order to get the service back up and running quickly. The discarded data is copied in logs that can be replayed.\n\n**19:20:** The service is operating normally, but large chunks of recent data are missing from graphs.\n\n**19:30:** The on call engineer begins replaying a portion of the logs to restore the missing data, while a second engineer begins looking for the root cause. Normally we'd wait a bit before doing root cause analysis, but without a good understanding of the cause we felt it could happen again soon.\n\n**20:57:** Half of the restore has been completed.\n\n**22:00:** All restores complete. The only missing data is that which was lost during the actual outage, which lasted for 30 minutes.\n\n## Actions already taken\nWe have added better error handling to the aggregation services to fix this condition.\n\n## Plan\n1) While we were able to replay logs to restore some data, we didn't record enough to allow us to"
    ],
    [
      "# Hosted Graphite Postmortem Report (#2)\n\n## Outage\n**Period:** March 12th 2014 18:50 UTC to 19:20 (30 minutes)  \n**Impact:** Complete loss of new incoming data from all customers in the shared environment on all interfaces (UDP, TCP, TCP python pickle, HTTP)  \n**Additional Impact:** Significant frontend graph rendering latency, Frontend HTTP 500 errors\n\n## Timeline (all times are UTC, which is also local for the responding engineers)\n\n**18:46:** A user, still sending data to us, deletes their account.\n\n**18:50:** All 13 aggregation services attempt to remove some data for this user from a queue, but a lookup on the user's ID fails. Queue processing stops, and it starts building up in memory. No new metric data is written out.\n\n**18:56:** PagerDuty fires an alert and calls the on-call engineer.\n\n**18:56:** The on call engineer begins investigating.\n\n**19:00:** The first outage notification tweet is sent.\n\n**19:10:** After spending ten minutes investigating and attempting to recover the service, a decision is made to discard a large amount of data in memory in order to get the service back up and running quickly. The discarded data is copied in logs that can be replayed.\n\n**19:20:** The service is operating normally, but large chunks of recent data are missing from graphs.\n\n**19:30:** The on call engineer begins replaying a portion of the logs to restore the missing data, while a second engineer begins looking for the root cause. Normally we'd wait a bit before doing root cause analysis, but without a good understanding of the cause we felt it could happen again soon.\n\n**20:57:** Half of the restore has been completed.\n\n**22:00:** All restores complete. The only missing data is that which was lost during the actual outage, which lasted for 30 minutes.\n\n## Actions already taken\nWe have added better error handling to the aggregation services to fix this condition.\n\n## Plan\n1) While we were able to replay logs to restore some data, we didn't record enough to allow us to"
    ],
    [
      "# Hosted Graphite Postmortem Report (#2)\n\n## Outage\n**Period:** March 12th 2014 18:50 UTC to 19:20 (30 minutes)  \n**Impact:** Complete loss of new incoming data from all customers in the shared environment on all interfaces (UDP, TCP, TCP python pickle, HTTP)  \n**Additional Impact:** Significant frontend graph rendering latency, Frontend HTTP 500 errors\n\n## Timeline (all times are UTC, which is also local for the responding engineers)\n\n**18:46:** A user, still sending data to us, deletes their account.\n\n**18:50:** All 13 aggregation services attempt to remove some data for this user from a queue, but a lookup on the user's ID fails. Queue processing stops, and it starts building up in memory. No new metric data is written out.\n\n**18:56:** PagerDuty fires an alert and calls the on-call engineer.\n\n**18:56:** The on call engineer begins investigating.\n\n**19:00:** The first outage notification tweet is sent.\n\n**19:10:** After spending ten minutes investigating and attempting to recover the service, a decision is made to discard a large amount of data in memory in order to get the service back up and running quickly. The discarded data is copied in logs that can be replayed.\n\n**19:20:** The service is operating normally, but large chunks of recent data are missing from graphs.\n\n**19:30:** The on call engineer begins replaying a portion of the logs to restore the missing data, while a second engineer begins looking for the root cause. Normally we'd wait a bit before doing root cause analysis, but without a good understanding of the cause we felt it could happen again soon.\n\n**20:57:** Half of the restore has been completed.\n\n**22:00:** All restores complete. The only missing data is that which was lost during the actual outage, which lasted for 30 minutes.\n\n## Actions already taken\nWe have added better error handling to the aggregation services to fix this condition.\n\n## Plan\n1) While we were able to replay logs to restore some data, we didn't record enough to allow us to"
    ],
    [
      "# Hosted Graphite Postmortem Report (#2)\n\n## Outage\n**Period:** March 12th 2014 18:50 UTC to 19:20 (30 minutes)  \n**Impact:** Complete loss of new incoming data from all customers in the shared environment on all interfaces (UDP, TCP, TCP python pickle, HTTP)  \n**Additional Impact:** Significant frontend graph rendering latency, Frontend HTTP 500 errors\n\n## Timeline (all times are UTC, which is also local for the responding engineers)\n\n**18:46:** A user, still sending data to us, deletes their account.\n\n**18:50:** All 13 aggregation services attempt to remove some data for this user from a queue, but a lookup on the user's ID fails. Queue processing stops, and it starts building up in memory. No new metric data is written out.\n\n**18:56:** PagerDuty fires an alert and calls the on-call engineer.\n\n**18:56:** The on call engineer begins investigating.\n\n**19:00:** The first outage notification tweet is sent.\n\n**19:10:** After spending ten minutes investigating and attempting to recover the service, a decision is made to discard a large amount of data in memory in order to get the service back up and running quickly. The discarded data is copied in logs that can be replayed.\n\n**19:20:** The service is operating normally, but large chunks of recent data are missing from graphs.\n\n**19:30:** The on call engineer begins replaying a portion of the logs to restore the missing data, while a second engineer begins looking for the root cause. Normally we'd wait a bit before doing root cause analysis, but without a good understanding of the cause we felt it could happen again soon.\n\n**20:57:** Half of the restore has been completed.\n\n**22:00:** All restores complete. The only missing data is that which was lost during the actual outage, which lasted for 30 minutes.\n\n## Actions already taken\nWe have added better error handling to the aggregation services to fix this condition.\n\n## Plan\n1) While we were able to replay logs to restore some data, we didn't record enough to allow us to"
    ],
    [
      "# Hosted Graphite Postmortem Report (#2)\n\n## Outage\n**Period:** March 12th 2014 18:50 UTC to 19:20 (30 minutes)  \n**Impact:** Complete loss of new incoming data from all customers in the shared environment on all interfaces (UDP, TCP, TCP python pickle, HTTP)  \n**Additional Impact:** Significant frontend graph rendering latency, Frontend HTTP 500 errors\n\n## Timeline (all times are UTC, which is also local for the responding engineers)\n\n**18:46:** A user, still sending data to us, deletes their account.\n\n**18:50:** All 13 aggregation services attempt to remove some data for this user from a queue, but a lookup on the user's ID fails. Queue processing stops, and it starts building up in memory. No new metric data is written out.\n\n**18:56:** PagerDuty fires an alert and calls the on-call engineer.\n\n**18:56:** The on call engineer begins investigating.\n\n**19:00:** The first outage notification tweet is sent.\n\n**19:10:** After spending ten minutes investigating and attempting to recover the service, a decision is made to discard a large amount of data in memory in order to get the service back up and running quickly. The discarded data is copied in logs that can be replayed.\n\n**19:20:** The service is operating normally, but large chunks of recent data are missing from graphs.\n\n**19:30:** The on call engineer begins replaying a portion of the logs to restore the missing data, while a second engineer begins looking for the root cause. Normally we'd wait a bit before doing root cause analysis, but without a good understanding of the cause we felt it could happen again soon.\n\n**20:57:** Half of the restore has been completed.\n\n**22:00:** All restores complete. The only missing data is that which was lost during the actual outage, which lasted for 30 minutes.\n\n## Actions already taken\nWe have added better error handling to the aggregation services to fix this condition.\n\n## Plan\n1) While we were able to replay logs to restore some data, we didn't record enough to allow us to"
    ],
    [
      ""
    ],
    [
      ""
    ],
    [
      ""
    ],
    [
      ""
    ],
    [
      ""
    ],
    [
      ""
    ],
    [
      ""
    ],
    [
      ""
    ],
    [
      ""
    ],
    [
      ""
    ],
    [
      "# Shakespeare Sonnet++ Postmortem (incident #465)\n\n## Date\n\n2015-10-21\n\n## Authors\n\n* jennifer\n* martym\n* agoogler\n\n## Status\n\nComplete, action items in progress\n\n## Summary\n\nShakespeare Search down for 66 minutes during period of very high interest in Shakespeare due to discovery of a new sonnet.\n\n## Impact\n\nEstimated 1.21B queries lost, no revenue impact.\n\n## Root Causes\n\nCascading failure due to combination of exceptionally high load and a resource leak when searches failed due to terms not being in the Shakespeare corpus. The newly discovered sonnet used a word that had never before appeared in one of Shakespeare's works, which happened to be the term users searched for. Under normal circumstances, the rate of task failures due to resource leaks is low enough to be unnoticed.\n\n## Trigger\n\nLatent bug triggered by sudden increase in traffic.\n\n## Resolution\n\nDirected traffic to sacrificial cluster and added 10x capacity to mitigate cascading failure. Updated index deployed, resolving interaction with latent bug. Maintaining extra capacity until surge in public interest in new sonnet passes. Resource leak identified and fix deployed.\n\n## Detection\n\nBorgmon detected high level of HTTP 500s and paged on-call.\n\n## Action Items\n\n| Action Item | Type | Owner | Bug | Status |\n|-------------|------|-------|-----|--------|\n| Update playbook with instructions for responding to cascading failure | mitigate | jennifer | n/a | **DONE** |\n| Use flux capacitor to balance load between clusters | prevent | martym | Bug 5554823 | **TODO** |\n| Schedule cascading failure test during next DiRT | process | docbrown | n/a | **TODO** |\n| Investigate running index MR/fusion continuously | prevent | jennifer | Bug 5554824 | **TODO** |\n| Plug file descriptor leak in search ranking subsystem | prevent | agoogler | Bug 5554825 | **DONE** |\n| Add load shedding capabilities to Shakespeare search | prevent | agoogler | Bug 5554826 | **TODO** |\n| Build regression tests to ensure servers respond sa"
    ],
    [
      "# Shakespeare Sonnet++ Postmortem (incident #465)\n\n## Date\n\n2015-10-21\n\n## Authors\n\n* jennifer\n* martym\n* agoogler\n\n## Status\n\nComplete, action items in progress\n\n## Summary\n\nShakespeare Search down for 66 minutes during period of very high interest in Shakespeare due to discovery of a new sonnet.\n\n## Impact\n\nEstimated 1.21B queries lost, no revenue impact.\n\n## Root Causes\n\nCascading failure due to combination of exceptionally high load and a resource leak when searches failed due to terms not being in the Shakespeare corpus. The newly discovered sonnet used a word that had never before appeared in one of Shakespeare's works, which happened to be the term users searched for. Under normal circumstances, the rate of task failures due to resource leaks is low enough to be unnoticed.\n\n## Trigger\n\nLatent bug triggered by sudden increase in traffic.\n\n## Resolution\n\nDirected traffic to sacrificial cluster and added 10x capacity to mitigate cascading failure. Updated index deployed, resolving interaction with latent bug. Maintaining extra capacity until surge in public interest in new sonnet passes. Resource leak identified and fix deployed.\n\n## Detection\n\nBorgmon detected high level of HTTP 500s and paged on-call.\n\n## Action Items\n\n| Action Item | Type | Owner | Bug | Status |\n|-------------|------|-------|-----|--------|\n| Update playbook with instructions for responding to cascading failure | mitigate | jennifer | n/a | **DONE** |\n| Use flux capacitor to balance load between clusters | prevent | martym | Bug 5554823 | **TODO** |\n| Schedule cascading failure test during next DiRT | process | docbrown | n/a | **TODO** |\n| Investigate running index MR/fusion continuously | prevent | jennifer | Bug 5554824 | **TODO** |\n| Plug file descriptor leak in search ranking subsystem | prevent | agoogler | Bug 5554825 | **DONE** |\n| Add load shedding capabilities to Shakespeare search | prevent | agoogler | Bug 5554826 | **TODO** |\n| Build regression tests to ensure servers respond sa"
    ],
    [
      "# Shakespeare Sonnet++ Postmortem (incident #465)\n\n## Date\n\n2015-10-21\n\n## Authors\n\n* jennifer\n* martym\n* agoogler\n\n## Status\n\nComplete, action items in progress\n\n## Summary\n\nShakespeare Search down for 66 minutes during period of very high interest in Shakespeare due to discovery of a new sonnet.\n\n## Impact\n\nEstimated 1.21B queries lost, no revenue impact.\n\n## Root Causes\n\nCascading failure due to combination of exceptionally high load and a resource leak when searches failed due to terms not being in the Shakespeare corpus. The newly discovered sonnet used a word that had never before appeared in one of Shakespeare's works, which happened to be the term users searched for. Under normal circumstances, the rate of task failures due to resource leaks is low enough to be unnoticed.\n\n## Trigger\n\nLatent bug triggered by sudden increase in traffic.\n\n## Resolution\n\nDirected traffic to sacrificial cluster and added 10x capacity to mitigate cascading failure. Updated index deployed, resolving interaction with latent bug. Maintaining extra capacity until surge in public interest in new sonnet passes. Resource leak identified and fix deployed.\n\n## Detection\n\nBorgmon detected high level of HTTP 500s and paged on-call.\n\n## Action Items\n\n| Action Item | Type | Owner | Bug | Status |\n|-------------|------|-------|-----|--------|\n| Update playbook with instructions for responding to cascading failure | mitigate | jennifer | n/a | **DONE** |\n| Use flux capacitor to balance load between clusters | prevent | martym | Bug 5554823 | **TODO** |\n| Schedule cascading failure test during next DiRT | process | docbrown | n/a | **TODO** |\n| Investigate running index MR/fusion continuously | prevent | jennifer | Bug 5554824 | **TODO** |\n| Plug file descriptor leak in search ranking subsystem | prevent | agoogler | Bug 5554825 | **DONE** |\n| Add load shedding capabilities to Shakespeare search | prevent | agoogler | Bug 5554826 | **TODO** |\n| Build regression tests to ensure servers respond sa"
    ],
    [
      "# Shakespeare Sonnet++ Postmortem (incident #465)\n\n## Date\n\n2015-10-21\n\n## Authors\n\n* jennifer\n* martym\n* agoogler\n\n## Status\n\nComplete, action items in progress\n\n## Summary\n\nShakespeare Search down for 66 minutes during period of very high interest in Shakespeare due to discovery of a new sonnet.\n\n## Impact\n\nEstimated 1.21B queries lost, no revenue impact.\n\n## Root Causes\n\nCascading failure due to combination of exceptionally high load and a resource leak when searches failed due to terms not being in the Shakespeare corpus. The newly discovered sonnet used a word that had never before appeared in one of Shakespeare's works, which happened to be the term users searched for. Under normal circumstances, the rate of task failures due to resource leaks is low enough to be unnoticed.\n\n## Trigger\n\nLatent bug triggered by sudden increase in traffic.\n\n## Resolution\n\nDirected traffic to sacrificial cluster and added 10x capacity to mitigate cascading failure. Updated index deployed, resolving interaction with latent bug. Maintaining extra capacity until surge in public interest in new sonnet passes. Resource leak identified and fix deployed.\n\n## Detection\n\nBorgmon detected high level of HTTP 500s and paged on-call.\n\n## Action Items\n\n| Action Item | Type | Owner | Bug | Status |\n|-------------|------|-------|-----|--------|\n| Update playbook with instructions for responding to cascading failure | mitigate | jennifer | n/a | **DONE** |\n| Use flux capacitor to balance load between clusters | prevent | martym | Bug 5554823 | **TODO** |\n| Schedule cascading failure test during next DiRT | process | docbrown | n/a | **TODO** |\n| Investigate running index MR/fusion continuously | prevent | jennifer | Bug 5554824 | **TODO** |\n| Plug file descriptor leak in search ranking subsystem | prevent | agoogler | Bug 5554825 | **DONE** |\n| Add load shedding capabilities to Shakespeare search | prevent | agoogler | Bug 5554826 | **TODO** |\n| Build regression tests to ensure servers respond sa"
    ],
    [
      "# Shakespeare Sonnet++ Postmortem (incident #465)\n\n## Date\n\n2015-10-21\n\n## Authors\n\n* jennifer\n* martym\n* agoogler\n\n## Status\n\nComplete, action items in progress\n\n## Summary\n\nShakespeare Search down for 66 minutes during period of very high interest in Shakespeare due to discovery of a new sonnet.\n\n## Impact\n\nEstimated 1.21B queries lost, no revenue impact.\n\n## Root Causes\n\nCascading failure due to combination of exceptionally high load and a resource leak when searches failed due to terms not being in the Shakespeare corpus. The newly discovered sonnet used a word that had never before appeared in one of Shakespeare's works, which happened to be the term users searched for. Under normal circumstances, the rate of task failures due to resource leaks is low enough to be unnoticed.\n\n## Trigger\n\nLatent bug triggered by sudden increase in traffic.\n\n## Resolution\n\nDirected traffic to sacrificial cluster and added 10x capacity to mitigate cascading failure. Updated index deployed, resolving interaction with latent bug. Maintaining extra capacity until surge in public interest in new sonnet passes. Resource leak identified and fix deployed.\n\n## Detection\n\nBorgmon detected high level of HTTP 500s and paged on-call.\n\n## Action Items\n\n| Action Item | Type | Owner | Bug | Status |\n|-------------|------|-------|-----|--------|\n| Update playbook with instructions for responding to cascading failure | mitigate | jennifer | n/a | **DONE** |\n| Use flux capacitor to balance load between clusters | prevent | martym | Bug 5554823 | **TODO** |\n| Schedule cascading failure test during next DiRT | process | docbrown | n/a | **TODO** |\n| Investigate running index MR/fusion continuously | prevent | jennifer | Bug 5554824 | **TODO** |\n| Plug file descriptor leak in search ranking subsystem | prevent | agoogler | Bug 5554825 | **DONE** |\n| Add load shedding capabilities to Shakespeare search | prevent | agoogler | Bug 5554826 | **TODO** |\n| Build regression tests to ensure servers respond sa"
    ],
    [
      "# Shakespeare Sonnet++ Postmortem (incident #465)\n\n## Date\n\n2015-10-21\n\n## Authors\n\n* jennifer\n* martym\n* agoogler\n\n## Status\n\nComplete, action items in progress\n\n## Summary\n\nShakespeare Search down for 66 minutes during period of very high interest in Shakespeare due to discovery of a new sonnet.\n\n## Impact\n\nEstimated 1.21B queries lost, no revenue impact.\n\n## Root Causes\n\nCascading failure due to combination of exceptionally high load and a resource leak when searches failed due to terms not being in the Shakespeare corpus. The newly discovered sonnet used a word that had never before appeared in one of Shakespeare's works, which happened to be the term users searched for. Under normal circumstances, the rate of task failures due to resource leaks is low enough to be unnoticed.\n\n## Trigger\n\nLatent bug triggered by sudden increase in traffic.\n\n## Resolution\n\nDirected traffic to sacrificial cluster and added 10x capacity to mitigate cascading failure. Updated index deployed, resolving interaction with latent bug. Maintaining extra capacity until surge in public interest in new sonnet passes. Resource leak identified and fix deployed.\n\n## Detection\n\nBorgmon detected high level of HTTP 500s and paged on-call.\n\n## Action Items\n\n| Action Item | Type | Owner | Bug | Status |\n|-------------|------|-------|-----|--------|\n| Update playbook with instructions for responding to cascading failure | mitigate | jennifer | n/a | **DONE** |\n| Use flux capacitor to balance load between clusters | prevent | martym | Bug 5554823 | **TODO** |\n| Schedule cascading failure test during next DiRT | process | docbrown | n/a | **TODO** |\n| Investigate running index MR/fusion continuously | prevent | jennifer | Bug 5554824 | **TODO** |\n| Plug file descriptor leak in search ranking subsystem | prevent | agoogler | Bug 5554825 | **DONE** |\n| Add load shedding capabilities to Shakespeare search | prevent | agoogler | Bug 5554826 | **TODO** |\n| Build regression tests to ensure servers respond sa"
    ],
    [
      "# Shakespeare Sonnet++ Postmortem (incident #465)\n\n## Date\n\n2015-10-21\n\n## Authors\n\n* jennifer\n* martym\n* agoogler\n\n## Status\n\nComplete, action items in progress\n\n## Summary\n\nShakespeare Search down for 66 minutes during period of very high interest in Shakespeare due to discovery of a new sonnet.\n\n## Impact\n\nEstimated 1.21B queries lost, no revenue impact.\n\n## Root Causes\n\nCascading failure due to combination of exceptionally high load and a resource leak when searches failed due to terms not being in the Shakespeare corpus. The newly discovered sonnet used a word that had never before appeared in one of Shakespeare's works, which happened to be the term users searched for. Under normal circumstances, the rate of task failures due to resource leaks is low enough to be unnoticed.\n\n## Trigger\n\nLatent bug triggered by sudden increase in traffic.\n\n## Resolution\n\nDirected traffic to sacrificial cluster and added 10x capacity to mitigate cascading failure. Updated index deployed, resolving interaction with latent bug. Maintaining extra capacity until surge in public interest in new sonnet passes. Resource leak identified and fix deployed.\n\n## Detection\n\nBorgmon detected high level of HTTP 500s and paged on-call.\n\n## Action Items\n\n| Action Item | Type | Owner | Bug | Status |\n|-------------|------|-------|-----|--------|\n| Update playbook with instructions for responding to cascading failure | mitigate | jennifer | n/a | **DONE** |\n| Use flux capacitor to balance load between clusters | prevent | martym | Bug 5554823 | **TODO** |\n| Schedule cascading failure test during next DiRT | process | docbrown | n/a | **TODO** |\n| Investigate running index MR/fusion continuously | prevent | jennifer | Bug 5554824 | **TODO** |\n| Plug file descriptor leak in search ranking subsystem | prevent | agoogler | Bug 5554825 | **DONE** |\n| Add load shedding capabilities to Shakespeare search | prevent | agoogler | Bug 5554826 | **TODO** |\n| Build regression tests to ensure servers respond sa"
    ],
    [
      "# Shakespeare Sonnet++ Postmortem (incident #465)\n\n## Date\n\n2015-10-21\n\n## Authors\n\n* jennifer\n* martym\n* agoogler\n\n## Status\n\nComplete, action items in progress\n\n## Summary\n\nShakespeare Search down for 66 minutes during period of very high interest in Shakespeare due to discovery of a new sonnet.\n\n## Impact\n\nEstimated 1.21B queries lost, no revenue impact.\n\n## Root Causes\n\nCascading failure due to combination of exceptionally high load and a resource leak when searches failed due to terms not being in the Shakespeare corpus. The newly discovered sonnet used a word that had never before appeared in one of Shakespeare's works, which happened to be the term users searched for. Under normal circumstances, the rate of task failures due to resource leaks is low enough to be unnoticed.\n\n## Trigger\n\nLatent bug triggered by sudden increase in traffic.\n\n## Resolution\n\nDirected traffic to sacrificial cluster and added 10x capacity to mitigate cascading failure. Updated index deployed, resolving interaction with latent bug. Maintaining extra capacity until surge in public interest in new sonnet passes. Resource leak identified and fix deployed.\n\n## Detection\n\nBorgmon detected high level of HTTP 500s and paged on-call.\n\n## Action Items\n\n| Action Item | Type | Owner | Bug | Status |\n|-------------|------|-------|-----|--------|\n| Update playbook with instructions for responding to cascading failure | mitigate | jennifer | n/a | **DONE** |\n| Use flux capacitor to balance load between clusters | prevent | martym | Bug 5554823 | **TODO** |\n| Schedule cascading failure test during next DiRT | process | docbrown | n/a | **TODO** |\n| Investigate running index MR/fusion continuously | prevent | jennifer | Bug 5554824 | **TODO** |\n| Plug file descriptor leak in search ranking subsystem | prevent | agoogler | Bug 5554825 | **DONE** |\n| Add load shedding capabilities to Shakespeare search | prevent | agoogler | Bug 5554826 | **TODO** |\n| Build regression tests to ensure servers respond sa"
    ],
    [
      "# Shakespeare Sonnet++ Postmortem (incident #465)\n\n## Date\n\n2015-10-21\n\n## Authors\n\n* jennifer\n* martym\n* agoogler\n\n## Status\n\nComplete, action items in progress\n\n## Summary\n\nShakespeare Search down for 66 minutes during period of very high interest in Shakespeare due to discovery of a new sonnet.\n\n## Impact\n\nEstimated 1.21B queries lost, no revenue impact.\n\n## Root Causes\n\nCascading failure due to combination of exceptionally high load and a resource leak when searches failed due to terms not being in the Shakespeare corpus. The newly discovered sonnet used a word that had never before appeared in one of Shakespeare's works, which happened to be the term users searched for. Under normal circumstances, the rate of task failures due to resource leaks is low enough to be unnoticed.\n\n## Trigger\n\nLatent bug triggered by sudden increase in traffic.\n\n## Resolution\n\nDirected traffic to sacrificial cluster and added 10x capacity to mitigate cascading failure. Updated index deployed, resolving interaction with latent bug. Maintaining extra capacity until surge in public interest in new sonnet passes. Resource leak identified and fix deployed.\n\n## Detection\n\nBorgmon detected high level of HTTP 500s and paged on-call.\n\n## Action Items\n\n| Action Item | Type | Owner | Bug | Status |\n|-------------|------|-------|-----|--------|\n| Update playbook with instructions for responding to cascading failure | mitigate | jennifer | n/a | **DONE** |\n| Use flux capacitor to balance load between clusters | prevent | martym | Bug 5554823 | **TODO** |\n| Schedule cascading failure test during next DiRT | process | docbrown | n/a | **TODO** |\n| Investigate running index MR/fusion continuously | prevent | jennifer | Bug 5554824 | **TODO** |\n| Plug file descriptor leak in search ranking subsystem | prevent | agoogler | Bug 5554825 | **DONE** |\n| Add load shedding capabilities to Shakespeare search | prevent | agoogler | Bug 5554826 | **TODO** |\n| Build regression tests to ensure servers respond sa"
    ],
    [
      "# Shakespeare Sonnet++ Postmortem (incident #465)\n\n## Date\n\n2015-10-21\n\n## Authors\n\n* jennifer\n* martym\n* agoogler\n\n## Status\n\nComplete, action items in progress\n\n## Summary\n\nShakespeare Search down for 66 minutes during period of very high interest in Shakespeare due to discovery of a new sonnet.\n\n## Impact\n\nEstimated 1.21B queries lost, no revenue impact.\n\n## Root Causes\n\nCascading failure due to combination of exceptionally high load and a resource leak when searches failed due to terms not being in the Shakespeare corpus. The newly discovered sonnet used a word that had never before appeared in one of Shakespeare's works, which happened to be the term users searched for. Under normal circumstances, the rate of task failures due to resource leaks is low enough to be unnoticed.\n\n## Trigger\n\nLatent bug triggered by sudden increase in traffic.\n\n## Resolution\n\nDirected traffic to sacrificial cluster and added 10x capacity to mitigate cascading failure. Updated index deployed, resolving interaction with latent bug. Maintaining extra capacity until surge in public interest in new sonnet passes. Resource leak identified and fix deployed.\n\n## Detection\n\nBorgmon detected high level of HTTP 500s and paged on-call.\n\n## Action Items\n\n| Action Item | Type | Owner | Bug | Status |\n|-------------|------|-------|-----|--------|\n| Update playbook with instructions for responding to cascading failure | mitigate | jennifer | n/a | **DONE** |\n| Use flux capacitor to balance load between clusters | prevent | martym | Bug 5554823 | **TODO** |\n| Schedule cascading failure test during next DiRT | process | docbrown | n/a | **TODO** |\n| Investigate running index MR/fusion continuously | prevent | jennifer | Bug 5554824 | **TODO** |\n| Plug file descriptor leak in search ranking subsystem | prevent | agoogler | Bug 5554825 | **DONE** |\n| Add load shedding capabilities to Shakespeare search | prevent | agoogler | Bug 5554826 | **TODO** |\n| Build regression tests to ensure servers respond sa"
    ]
  ],
  "answers": [
    "The root cause of the outage was the deletion of a user's account, which led to all 13 aggregation services attempting to remove some data for this user from a queue. This caused a lookup failure on the user's ID, resulting in queue processing stopping and building up in memory, preventing new metric data from being written out.",
    "The root cause of the queue processing failure was that a user's account deletion caused a lookup on the user's ID to fail during the data removal process by the aggregation services, leading to a halt in queue processing.",
    "To resolve the system failure, the on-call engineer decided to discard a large amount of data in memory to quickly get the service back up and running. This discarded data was copied into logs that could be replayed. The engineer then began replaying a portion of the logs to restore the missing data, while a second engineer started investigating the root cause.",
    "The on-call engineer decided to discard a large amount of data in memory to get the service back up and running quickly, with the discarded data being copied into logs for potential replay.",
    "The impact of the outage included a complete loss of new incoming data from all customers in the shared environment across all interfaces (UDP, TCP, TCP python pickle, HTTP). Additionally, there was significant frontend graph rendering latency and Frontend HTTP 500 errors.",
    "During the outage, there was a complete loss of new incoming data from all customers in the shared environment across all interfaces. Additionally, there was significant frontend graph rendering latency and HTTP 500 errors.",
    "Implementing more robust error handling in the aggregation services could have prevented the failure in processing the queue. This has already been addressed by adding better error handling to fix this condition.",
    "The postmortem recommended adding better error handling to the aggregation services to prevent queue processing from stopping due to a failed user ID lookup. Additionally, a project has been prioritized to record all incoming data, which would allow for complete recovery from future outages by replaying logs.",
    "The system failure was detected by an alert from PagerDuty, which fired at 18:56 UTC and called the on-call engineer.",
    "PagerDuty fired an alert at 18:56 UTC, notifying the on-call engineer of the outage.",
    "The root cause of the service outage was a misconfiguration in the database settings, specifically an incorrect timeout setting that was too low, leading to premature disconnections.",
    "The root cause of the service outage on September 15th was a misconfiguration in the load balancer settings that inadvertently routed traffic to a deprecated server pool, which could not handle the current traffic volume.",
    "The resolution involved restarting the database server and increasing the connection pool size to handle the increased load more efficiently.",
    "The on-call team identified the misconfiguration in the database settings that caused the service outage. They reverted the configuration change to its last known good state, performed a thorough validation of the changes, and monitored the system for any further anomalies.",
    "During the system failure incident, 25% of customer transactions were delayed, resulting in approximately 500 transactions being processed late. No transactions were lost, but processing delays led to a temporary backlog.",
    "Approximately 15,000 users were affected by the service outage.",
    "Implementing automatic scaling policies and conducting regular load testing could have helped in handling the unexpected surge in traffic, preventing the database from becoming overwhelmed.",
    "The postmortem recommended increasing the size of the database connection pool, implementing more robust connection timeout settings, and adding monitoring alerts for connection pool usage to prevent future outages.",
    "The system outage was detected through automated alerts from our monitoring system, which flagged a significant increase in error rates and a drop in service availability.",
    "During the service outage on September 15th, the alerts that fired included 'Service Unavailable Error Rate' and 'Database Connection Timeouts'. These alerts were triggered by the elevated error rates and increased latency observed in the database operations.",
    "The root cause of the system failure was a cascading failure due to a combination of exceptionally high load and a resource leak when searches failed due to terms not being in the Shakespeare corpus. The newly discovered sonnet contained a word that had never appeared in any of Shakespeare's works, which users searched for and triggered a latent bug.",
    "The service failure was caused by a cascading failure due to a combination of exceptionally high load and a resource leak. The incident was triggered by searches for a newly discovered sonnet that included a word not previously in the Shakespeare corpus, leading to task failures when searches did not find the term.",
    "To resolve the cascading failure, traffic was directed to a sacrificial cluster and the capacity was increased by 10x to prevent the failure from spreading further. Additionally, an updated index was deployed to resolve the interaction with the latent bug, and the resource leak was identified and fixed. The team is maintaining extra capacity until the surge in public interest in the new sonnet passes.",
    "Traffic was directed to a sacrificial cluster and capacity was increased by 10x to manage the cascading failure. An updated index was deployed to resolve the interaction with the latent bug. Additionally, a resource leak was identified and a fix was deployed to address it.",
    "The impact of the outage was an estimated 1.21 billion lost queries, although there was no revenue impact.",
    "An estimated 1.21 billion queries were lost during the outage.",
    "To prevent the cascading failure, several measures could have been implemented: 1) Add load shedding capabilities to the Shakespeare search to handle unexpected surges in traffic effectively. 2) Continuously run the index MR/fusion process to keep the data up-to-date and prevent failures related to missing terms. 3) Fix the file descriptor leak in the search ranking subsystem to prevent resource exhaustion during high load periods. 4) Build regression tests to ensure the servers handle unexpected queries without failing. These action items are identified in the postmortem and are either completed or in progress.",
    "The recommended preventive measures included using a flux capacitor to balance load between clusters, investigating continuous running of index MR/fusion, adding load shedding capabilities to Shakespeare search, and building regression tests to ensure servers respond sanely to queries of death.",
    "The system failure was detected by Borgmon, which identified a high level of HTTP 500 errors and subsequently paged the on-call engineer.",
    "Borgmon detected a high level of HTTP 500s and paged the on-call engineer."
  ],
  "ground_truths": [
    "The root cause of the outage was the deletion of a user's account, which led to all 13 aggregation services attempting to remove some data for this user from a queue. This caused a lookup failure on the user's ID, resulting in queue processing stopping and building up in memory, preventing new metric data from being written out.",
    "The root cause of the queue processing failure was that a user's account deletion caused a lookup on the user's ID to fail during the data removal process by the aggregation services, leading to a halt in queue processing.",
    "To resolve the system failure, the on-call engineer decided to discard a large amount of data in memory to quickly get the service back up and running. This discarded data was copied into logs that could be replayed. The engineer then began replaying a portion of the logs to restore the missing data, while a second engineer started investigating the root cause.",
    "The on-call engineer decided to discard a large amount of data in memory to get the service back up and running quickly, with the discarded data being copied into logs for potential replay.",
    "The impact of the outage included a complete loss of new incoming data from all customers in the shared environment across all interfaces (UDP, TCP, TCP python pickle, HTTP). Additionally, there was significant frontend graph rendering latency and Frontend HTTP 500 errors.",
    "During the outage, there was a complete loss of new incoming data from all customers in the shared environment across all interfaces. Additionally, there was significant frontend graph rendering latency and HTTP 500 errors.",
    "Implementing more robust error handling in the aggregation services could have prevented the failure in processing the queue. This has already been addressed by adding better error handling to fix this condition.",
    "The postmortem recommended adding better error handling to the aggregation services and prioritized a project to record all incoming data for complete recovery from future outages.",
    "The system failure was detected by an alert from PagerDuty, which fired at 18:56 UTC and called the on-call engineer.",
    "PagerDuty fires an alert and calls the on-call engineer.",
    "The root cause of the service outage was a misconfiguration in the database settings, specifically an incorrect timeout setting that was too low, leading to premature disconnections.",
    "The root cause of the service outage on September 15th was a misconfiguration in the load balancer settings that inadvertently routed traffic to a deprecated server pool, which could not handle the current traffic volume.",
    "The resolution involved restarting the database server and increasing the connection pool size to handle the increased load more efficiently.",
    "The on-call team identified the misconfiguration in the database settings that caused the service outage. They reverted the configuration change to its last known good state, performed a thorough validation of the changes, and monitored the system for any further anomalies.",
    "During the system failure incident, 25% of customer transactions were delayed, resulting in approximately 500 transactions being processed late. No transactions were lost, but processing delays led to a temporary backlog.",
    "Approximately 15,000 users were affected by the service outage.",
    "Implementing automatic scaling policies and conducting regular load testing could have helped in handling the unexpected surge in traffic, preventing the database from becoming overwhelmed.",
    "The postmortem recommended increasing the size of the database connection pool, implementing more robust connection timeout settings, and adding monitoring alerts for connection pool usage.",
    "The system outage was detected through automated alerts from our monitoring system, which flagged a significant increase in error rates and a drop in service availability.",
    "During the service outage on September 15th, the alerts that fired included 'Service Unavailable Error Rate' and 'Database Connection Timeouts'. These alerts were triggered by the elevated error rates and increased latency observed in the database operations.",
    "The root cause of the system failure was a cascading failure due to a combination of exceptionally high load and a resource leak when searches failed due to terms not being in the Shakespeare corpus. The newly discovered sonnet contained a word that had never appeared in any of Shakespeare's works, which users searched for and triggered a latent bug.",
    "The service failure was caused by a cascading failure due to a combination of exceptionally high load and a resource leak when searches failed because the newly discovered sonnet contained a word not previously in the Shakespeare corpus.",
    "Directed traffic to sacrificial cluster and added 10x capacity to mitigate cascading failure. Updated index deployed, resolving interaction with latent bug. Maintaining extra capacity until surge in public interest in new sonnet passes. Resource leak identified and fix deployed.",
    "Traffic was directed to a sacrificial cluster and capacity was increased by 10x to manage the cascading failure. An updated index was deployed to resolve the interaction with the latent bug. Additionally, a resource leak was identified and a fix was deployed to address it.",
    "The impact of the outage was an estimated 1.21 billion lost queries, although there was no revenue impact.",
    "An estimated 1.21 billion queries were lost during the outage.",
    "To prevent the incident, the team could implement load shedding capabilities, continuously run the index MR/fusion, fix the file descriptor leak, and build regression tests to handle unexpected queries. These measures are listed as action items in the postmortem.",
    "The preventive measures recommended were to use a flux capacitor to balance load between clusters, investigate running index MR/fusion continuously, plug the file descriptor leak in the search ranking subsystem, add load shedding capabilities to Shakespeare search, and build regression tests to ensure servers respond sanely to queries of death.",
    "Borgmon detected high level of HTTP 500s and paged on-call.",
    "Borgmon detected high level of HTTP 500s and paged on-call."
  ]
}