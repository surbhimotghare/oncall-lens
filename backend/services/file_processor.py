"""
File Processor Service for Oncall Lens
Handles processing of different file types for incident analysis.
"""

import logging
import mimetypes
import os
from pathlib import Path
from typing import List, Dict, Any
import hashlib

from fastapi import UploadFile, HTTPException
import aiofiles

from models.api_models import ProcessedFile
from config.settings import get_settings

logger = logging.getLogger(__name__)


class FileProcessor:
    """
    Service for processing uploaded incident files.
    Handles various file types: logs, diffs, stack traces, images, etc.
    """
    
    def __init__(self):
        self.settings = get_settings()
        self.max_file_size = self.settings.max_file_size  # Already in bytes
        self.supported_extensions = set(self.settings.allowed_file_types)
        
    async def process_files(self, files: List[UploadFile]) -> List[ProcessedFile]:
        """
        Process a list of uploaded files and extract their content.
        
        Args:
            files: List of uploaded files from FastAPI
            
        Returns:
            List of processed files with extracted content
        """
        processed_files = []
        
        for file in files:
            logger.info(f"ğŸ” Processing file: {file.filename}")
            try:
                processed_file = await self._process_single_file(file)
                processed_files.append(processed_file)
                logger.info(f"âœ… Successfully processed file: {file.filename} ({processed_file.file_type})")
                
            except HTTPException as e:
                logger.error(f"âŒ HTTP error processing file {file.filename}: {e.detail}")
                # For HTTP exceptions, we want to fail fast rather than continue
                raise e
                
            except Exception as e:
                logger.error(f"âŒ Unexpected error processing file {file.filename}: {type(e).__name__}: {e}")
                # For unexpected errors, create an error file but continue processing others
                processed_files.append(ProcessedFile(
                    filename=file.filename or "unknown",
                    file_type="error",
                    content=f"Failed to process: {str(e)}",
                    size_bytes=0,
                    processing_notes=f"Processing failed: {str(e)}"
                ))
                
        return processed_files
    
    async def _process_single_file(self, file: UploadFile) -> ProcessedFile:
        """
        Process a single uploaded file.
        
        Args:
            file: Single uploaded file
            
        Returns:
            ProcessedFile with extracted content
        """
        logger.info(f"ğŸ” Starting to process file: {file.filename}")
        
        try:
            # Validate file
            await self._validate_file(file)
            logger.info(f"ğŸ” File validation passed for: {file.filename}")
            
            # Reset file pointer to beginning and read content
            await file.seek(0)
            logger.info(f"ğŸ” File pointer reset for: {file.filename}")
            
            # Read file content
            content = await file.read()
            file_size = len(content)
            logger.info(f"ğŸ” File read - {file.filename}: size={file_size} bytes")
            
            # Validate file size and content
            if file_size == 0:
                logger.error(f"ğŸ” File is empty: {file.filename}")
                raise HTTPException(status_code=400, detail="File is empty")
            
            if file_size > self.max_file_size:
                logger.error(f"ğŸ” File too large: {file.filename} - {file_size} bytes")
                raise HTTPException(
                    status_code=400, 
                    detail=f"File too large: {file_size} bytes. Maximum allowed: {self.max_file_size} bytes"
                )
            
            # Debug logging - show actual content preview
            content_preview = content[:200] if isinstance(content, bytes) else str(content)[:200]
            logger.info(f"ğŸ” File content preview - {file.filename}: {content_preview}")
            
            # Determine file type and process accordingly
            file_type = self._determine_file_type(file.filename, content)
            logger.info(f"ğŸ” Determined file type: {file_type} for {file.filename}")
            
            # Extract text content based on file type
            text_content = await self._extract_content(content, file_type, file.filename)
            logger.info(f"ğŸ” Extracted content length: {len(text_content)} chars, type: {file_type}")
            
            return ProcessedFile(
                filename=file.filename or "unknown",
                file_type=file_type,
                content=text_content,
                size_bytes=file_size,
                processing_notes=f"Successfully processed as {file_type}"
            )
            
        except HTTPException:
            # Re-raise HTTP exceptions
            raise
        except Exception as e:
            logger.error(f"ğŸ” Unexpected error processing {file.filename}: {type(e).__name__}: {e}")
            logger.error(f"ğŸ” File state - filename: {file.filename}, content_type: {getattr(file, 'content_type', 'unknown')}")
            raise HTTPException(status_code=500, detail=f"Failed to process file: {str(e)}")
    
    async def _validate_file(self, file: UploadFile) -> None:
        """
        Validate uploaded file (size, type, etc.)
        
        Args:
            file: File to validate
            
        Raises:
            HTTPException: If file is invalid
        """
        if not file.filename:
            raise HTTPException(status_code=400, detail="Filename is required")
        
        # Check file extension
        file_ext = Path(file.filename).suffix.lower()
        if file_ext not in self.supported_extensions:
            raise HTTPException(
                status_code=400, 
                detail=f"Unsupported file type: {file_ext}. Supported types: {self.supported_extensions}"
            )
        
        # Don't read the file here - just check if we can access it
        # The actual size validation will happen after we read the full content
    
    def _determine_file_type(self, filename: str, content: bytes) -> str:
        """
        Determine the type of file based on filename and content.
        
        Args:
            filename: Original filename
            content: File content bytes
            
        Returns:
            String representing the file type
        """
        filename_lower = filename.lower()
        file_ext = Path(filename).suffix.lower()
        
        # Check by filename patterns
        if any(pattern in filename_lower for pattern in ['stack', 'trace', 'exception']):
            return "stack_trace"
        elif any(pattern in filename_lower for pattern in ['.log', 'error', 'debug']):
            return "log_file"
        elif any(pattern in filename_lower for pattern in ['.diff', '.patch']):
            return "code_diff"
        elif 'postmortem' in filename_lower or 'incident' in filename_lower:
            return "postmortem"
        elif any(pattern in filename_lower for pattern in ['cpu', 'memory', 'metrics', 'dashboard']):
            return "metrics"
        
        # Check by file extension
        if file_ext in ['.log']:
            return "log_file"
        elif file_ext in ['.diff', '.patch']:
            return "code_diff"
        elif file_ext in ['.md', '.txt'] and len(content) > 1000:
            return "documentation"
        elif file_ext in ['.json']:
            return "configuration"
        elif file_ext in ['.png', '.jpg', '.jpeg']:
            return "screenshot"
        elif file_ext in ['.csv']:
            return "metrics_data"
        else:
            return "text_file"
    
    async def _extract_content(self, content: bytes, file_type: str, filename: str) -> str:
        """
        Extract text content from file based on its type.
        
        Args:
            content: File content bytes
            file_type: Determined file type
            filename: Original filename
            
        Returns:
            Extracted text content
        """
        try:
            if file_type == "screenshot":
                return await self._process_image(content, filename)
            else:
                # Try to decode as text
                text_content = await self._decode_text(content)
                
                # Apply type-specific processing
                if file_type == "log_file":
                    return self._process_log_file(text_content)
                elif file_type == "stack_trace":
                    return self._process_stack_trace(text_content)
                elif file_type == "code_diff":
                    return self._process_code_diff(text_content)
                elif file_type == "configuration":
                    return self._process_json_config(text_content)
                else:
                    return text_content
                    
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to extract content from {filename}: {e}")
            return f"[Content extraction failed: {str(e)}]"
    
    async def _decode_text(self, content: bytes) -> str:
        """
        Decode bytes to text, trying multiple encodings.
        
        Args:
            content: Bytes to decode
            
        Returns:
            Decoded text string
        """
        if not content:
            return ""
            
        encodings = ['utf-8', 'utf-16', 'iso-8859-1', 'cp1252']
        
        for encoding in encodings:
            try:
                return content.decode(encoding)
            except UnicodeDecodeError:
                continue
        
        # If all fail, decode with errors ignored
        return content.decode('utf-8', errors='ignore')
    
    def _process_log_file(self, content: str) -> str:
        """
        Process log file content to highlight important information.
        
        Args:
            content: Raw log content
            
        Returns:
            Processed log content with annotations
        """
        lines = content.split('\n')
        processed_lines = []
        
        for line in lines:
            line_lower = line.lower()
            # Mark important lines
            if any(keyword in line_lower for keyword in ['error', 'exception', 'failed', 'timeout']):
                processed_lines.append(f"ğŸ”´ ERROR: {line}")
            elif any(keyword in line_lower for keyword in ['warn', 'warning']):
                processed_lines.append(f"ğŸŸ¡ WARN: {line}")
            elif any(keyword in line_lower for keyword in ['info', 'start', 'stop', 'success']):
                processed_lines.append(f"â„¹ï¸ INFO: {line}")
            else:
                processed_lines.append(line)
        
        return '\n'.join(processed_lines)
    
    def _process_stack_trace(self, content: str) -> str:
        """
        Process stack trace content to highlight the exception and key frames.
        
        Args:
            content: Raw stack trace content
            
        Returns:
            Processed stack trace with annotations
        """
        lines = content.split('\n')
        processed_lines = []
        
        for line in lines:
            if line.strip().startswith('Exception') or line.strip().startswith('Error'):
                processed_lines.append(f"ğŸ’¥ EXCEPTION: {line}")
            elif 'at ' in line and any(keyword in line for keyword in ['java.', 'com.', 'org.']):
                processed_lines.append(f"ğŸ“ FRAME: {line}")
            elif 'Caused by:' in line:
                processed_lines.append(f"ğŸ”— CAUSE: {line}")
            else:
                processed_lines.append(line)
        
        return '\n'.join(processed_lines)
    
    def _process_code_diff(self, content: str) -> str:
        """
        Process code diff to highlight changes.
        
        Args:
            content: Raw diff content
            
        Returns:
            Processed diff with annotations
        """
        lines = content.split('\n')
        processed_lines = []
        
        for line in lines:
            if line.startswith('+') and not line.startswith('+++'):
                processed_lines.append(f"â• ADDED: {line}")
            elif line.startswith('-') and not line.startswith('---'):
                processed_lines.append(f"â– REMOVED: {line}")
            elif line.startswith('@@'):
                processed_lines.append(f"ğŸ“ LOCATION: {line}")
            else:
                processed_lines.append(line)
        
        return '\n'.join(processed_lines)
    
    def _process_json_config(self, content: str) -> str:
        """
        Process JSON configuration files.
        
        Args:
            content: Raw JSON content
            
        Returns:
            Processed JSON with validation
        """
        try:
            import json
            parsed = json.loads(content)
            # Return pretty-printed JSON
            return json.dumps(parsed, indent=2)
        except json.JSONDecodeError as e:
            return f"âš ï¸ INVALID JSON: {content}\n\nError: {str(e)}"
    
    async def _process_image(self, content: bytes, filename: str) -> str:
        """
        Process image files (placeholder for OCR functionality).
        
        Args:
            content: Image bytes
            filename: Original filename
            
        Returns:
            Description of the image (placeholder)
        """
        # Placeholder for image processing
        # In a real implementation, you might use OCR libraries like pytesseract
        # or send to GPT-4 Vision API for analysis
        
        image_hash = hashlib.md5(content).hexdigest()[:8]
        
        return f"""
ğŸ“¸ IMAGE FILE: {filename}
ğŸ” Size: {len(content)} bytes
ğŸ†” Hash: {image_hash}

ğŸ“‹ CONTENT DESCRIPTION:
This appears to be a screenshot or image file related to the incident.
For full analysis, this image would need to be processed with OCR or 
vision AI capabilities.

ğŸ”§ PROCESSING NOTE:
Image content extraction is not yet implemented. Consider:
- Using OCR to extract text from screenshots
- Using GPT-4 Vision API to analyze dashboard screenshots
- Converting charts/graphs to textual descriptions
""" 